\documentclass[
    a4paper,
    man,
    natbib
]{apa6}

\usepackage[english]{babel}

% maps apacite commands to biblatex commands
\let \citeNP \cite
\let \citeA \textcite
\let \cite \parencite

\makeatletter

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option
% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts0
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[colorlinks, linkcolor=black, urlcolor=black, citecolor=black]{hyperref}

%=====================================================================
%============================ our packages ===========================

\usepackage{color}
\definecolor{black}{rgb}{0,0,0}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,positioning, fit, arrows.meta, shapes}
\usetikzlibrary{shapes}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{float}

%=====================================================================
%============================= citations =============================


% Flexible sec/fig/tbl/def cross-refs.
\newcommand{\Secref}[1]{Section~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\dashsecref}[2]{Sections~\ref{#1}--\ref{#2}}

\newcommand{\Defref}[1]{Definition~\ref{#1}}
\newcommand{\defref}[1]{Definition~\ref{#1}}
\newcommand{\Defrefc}[2]{\Defref{#1}, clause~\ref{#2}}
\newcommand{\defrefc}[2]{\defref{#1}, clause~\ref{#2}}

\newcommand{\Figref}[1]{Figure~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\dashfigref}[2]{Figures~\ref{#1}--\ref{#2}}
\newcommand{\Tabref}[1]{Table~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

%\newcommand{\Appref}[1]{Appendix~\ref{#1}}
%\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\Appref}[1]{SI}
\newcommand{\appref}[1]{SI}

% In-text citations
\newcommand{\posscitet}[1]{\citeauthor{#1}'s~(\citeyear{#1})}
\newcommand{\sposscitet}[1]{\citeauthor{#1}'~(\citeyear{#1})}
\newcommand{\possciteauthor}[1]{\citeauthor{#1}'s}
\newcommand{\spossciteauthor}[1]{\citeauthor{#1}'}
\newcommand{\pgposscitet}[2]{\citeauthor{#1}'s~(\citeyear{#1}:~#2)}
\newcommand{\secposscitet}[2]{\citeauthor{#1}'s~(\citeyear{#1}:~$\S$#2)}
\newcommand{\pgcitealt}[2]{\citealt{#1}:~#2}
\newcommand{\seccitealt}[2]{\citealt{#1}:~$\S$#2}
\newcommand{\pgcitep}[2]{(\citealt{#1}:~#2)}
\newcommand{\seccitep}[2]{(\citealt{#1}:~$\S$#2)}
\newcommand{\pgcitet}[2]{\citeauthor{#1}~(\citeyear{#1}:~#2)}
\newcommand{\seccitet}[2]{\citeauthor{#1}~(\citeyear{#1}:~$\S$#2)}

% Examples:
\newcommand{\eg}[1]{(\ref{#1})}
\newcommand{\dasheg}[2]{\eg{#1}--\eg{#2}}
\newcommand{\subeg}[2]{(\ref{#1}\ref{#2})}
\newcommand{\dblsubeg}[3]{(\ref{#1}\ref{#2},~\ref{#3})}
\newcommand{\dashsubeg}[3]{(\ref{#1}\ref{#2}--\ref{#3})}

%=====================================================================
%========================= font manipulation =========================

\newcommand{\word}[1]{\emph{#1}}
\newcommand{\tech}[1]{\emph{#1}}
\newcommand{\highlight}[1]{\textbf{#1}}

\newcommand{\blue}[1]{{\color{blue}#1}}

\definecolor{ourgreen}{HTML}{4D8951}
\definecolor{darkblue}{HTML}{0499CC}
\definecolor{darkred}{HTML}{8B0000}
\definecolor{superlightgray}{HTML}{B8B8B8}

\newcommand{\poscell}[1]{\fcolorbox{white}{ourgreen}{#1}}
\newcommand{\negcell}[1]{\fcolorbox{white}{superlightgray}{#1}}


% \newcommand{\update}[1]{{\color{darkblue}#1}}
% \newcommand{\updatea}[1]{{\color{darkred}#1}}
% \newcommand{\updateb}[1]{{\color{ourgreen}#1}}

\newcommand{\update}[1]{#1}
\newcommand{\updatea}[1]{#1}
\newcommand{\updateb}[1]{#1}

%=====================================================================
%================================ math ===============================

\newcommand{\softmax}{\mathbf{softmax}}
\DeclareMathOperator{\ReLU}{ReLU}
\newcommand{\identity}{\mathbb{I}}
\newcommand{\LSTM}{\textbf{LSTM}}
\newcommand{\wupdate}{\mathrel{{+}{=}}}



\newcommand{\inputcolor}{black!20}
\newcommand{\outputcolor}{red!40}
\newcommand{\hiddencolor}{blue!40}
\newcommand{\hiddencolortwo}{orange!40}
\newcommand{\colorlabel}{green!40}
\newcommand{\compare}{\emph{Euclidean}}


\newcommand{\cite}{\citep}

\newcommand{\dropcap}[1]{#1}

%=====================================================================
%=====================================================================


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{Authors }
%  Atticus Geiger, Alexandra Carstensen, Michael C.~Frank,Christopher Potts }
\title{Relational reasoning and generalization using non-symbolic neural networks}
\shorttitle{Relational reasoning with neural networks}
\affiliation{Affiliation}%Stanford University}
\abstract{The notion of equality, or identity, is simple and ubiquitous, making it a key case study for broader questions about the representations supporting abstract relational reasoning. Previous work suggested that neural networks were not suitable models of human relational reasoning because they could not represent equality. We revisit this  question. In our experiments, we assess out-of-sample generalization of equality using both arbitrary representations and representations that have been pretrained on separate tasks to imbue them with structure. We find neural networks are able to learn (1) basic equality, (2) sequential equality problems (learning ABA-patterned sequences) with only positive training instances, and (3) a complex, hierarchical equality problem with only basic equality training instances ("zero-shot" generalization). In the two latter cases, our models surpass benchmarks proposed to demarcate human-unique symbolic abilities. These results suggest that essential aspects of symbolic reasoning can emerge from data-driven, non-symbolic learning processes.}

\begin{document}

\maketitle

\section{Introduction}\label{sec:introduction}

\newcommand{\papermode}{arxiv}

\makeatother

\newcommand{\newatticus}[1]{{\color{red}#1}}
\newcommand{\newmike}[1]{{\color{red}#1}}
\newcommand{\newalex}[1]{{\color{red}#1}}
\newcommand{\methref}{Methods}
\newcommand{\SIref}{Appendices }

One of the key components of human intelligence is our ability to reason about abstract relations between stimuli. Many of the most unremarkable human activities -- scheduling a meeting, following traffic signs, assembling furniture -- require a fluency with abstraction and relational reasoning that is unmatched in nonhuman animals. An influential perspective on human uniqueness holds that relational concepts are critical to higher-order cognition \citep[e.g.,][]{Gentner:2003}. By far the most common case study of abstract relations has been equality.\footnote{We use the term ``equality'' here, though different literatures have also used ``identity.''} Equality is a valuable case study because it is simple and ubiquitous, but also completely abstract in the sense that it can be evaluated regardless of the identity of the stimuli being judged.

Equality reasoning has been studied extensively across a host of systems and tasks, with wildly variant conclusions. In some studies, equality is very challenging to learn: only great apes with either extensive language experience or specialized training succeed in matching tasks in which a \emph{same} pair, AA, must be matched to a novel same pair, BB \cite{Premack:1983,thompson:2001}. Preschool children also struggle to learn these regularities in a seemingly similar task \citep{walker:2016}. In contrast, other studies suggest that equality is simple: bees are able to learn abstract identity relationships from only a small set of training trials \cite{giurfa:2001,avargues:2011}, and human infants can generalize identity patterns \cite{anderson:2018} and succeed in relational matching tasks \cite{ferry:2015}. We take the central challenge of this literature to be characterizing the conditions that lead to success or failure in learning an abstract relation in a way that can be productively generalized to new stimuli \citep{carstensen:inpress}.

The learning task in all of these cases can be described using the predicate \emph{same} (or equivalently, =), which operates over two inputs and returns {\sc true} if they are identical in some respect. One perspective in the literature is that success in these learning tasks implies the presence of an equivalent symbolic description in the mind of the solver \cite{marcus:1999,Premack:1983}. This view does not provide a lever to distinguish which of these tasks are trivial and which are difficult, however. Further, it can fall prey to circularity: because newborns show sensitivity to identity relations \cite{gervain:2012}, then it would follow from this argument that they must have symbolic representations. If this logic applies also to bees, then we presuppose symbolic representations universally and have no account of the gradient difficulty of different tasks for different species.

An explanation of when same--different tasks are trivial and when they are difficult requires a theoretical framework beyond the symbolic/non-symbolic distinction. To make quantitative predictions about task performance, such a framework should ideally be instantiated in a computational model that takes in training data and learns a solution that generalizes when assessed with stimuli analogous to those used in experimental assessments. Symbolic computational models \cite[e.g.,][]{frank:2011} can be used to make contact with data about the breadth of generalization, but they require the existence of a symbolic equality predicate and hence again presuppose symbolic abilities in every case of success. Ideally, we would want a model that describes under what conditions \emph{same} is easy and under what conditions it is hard or unlearnable -- and how learning proceeds in hard cases. Here, we aim to lay the foundation for the development of such an account.

We are inspired by an emergent perspective in the animal learning literature that the representations underlying non-human animals' and human infants' successes in equality reasoning tasks are graded \cite{wasserman:2017}. This view acknowledges the increasing evidence that other species like pigeons \cite{cook2007}, crows \cite{smirnova2015}, and baboons \cite{fagot2011} can make true, out-of-sample generalizations of \emph{same} and \emph{different} relations, but it also recognizes that the observed patterns of behavior do not show the hallmarks of all-or-none symbolic representations. Instead, performance is graded. Out-of-sample generalization is possible but the level of performance depends critically on the diversity of the training stimuli \cite[e.g.,][]{castro2010}. Success requires hundreds, thousands, or even tens of thousands of training trials. And the outcome of learning is noisy and imperfect. These learning signatures appear to be a close match to the kind of learning exhibited by neural networks. Such networks are a flexible framework for arbitrary function learning, which have enjoyed a huge resurgence of interest in recent years in the fields of artificial intelligence, neuroscience, and cognitive science \cite[e.g.,][]{lecun2015,saxe2019}.

In an an influential rebuttal of the use of neural network models for capturing relational reasoning, Marcus et al. \cite{marcus:1999} argued that a broad class of recurrent neural networks were unable to learn equality relations. These claims were subsequently challenged by the presentation of evidence that some forms of neural networks are able to learn (at least aspects of) Marcus et al.'s equality tasks  \cite{dienes:1999,seidenberg:1999a,seidenberg:1999b,elman:1999,negishi:1999}, yet these examples were not uncontroversial. The resulting debate \cite[reviewed in][]{alhama:2019} revealed a striking lack of consensus on some of the ground rules regarding what sort of generalization would be required to show that the learned function was suitably abstract.

In the time since these debates, extraordinarily successful neural network models have been developed for tasks such as natural language inference \cite{Bowman:2015, Williams:2018}, question answering \cite{Rajpurkar:2016, Rajpurkar:2018}, or visual reasoning \cite{Johnson:2017}, all of which are far more complex than equality-based tasks. In light of these findings, it may be surprising that the debate over equality-based reasoning is unresolved \cite{alhama:2019}. Yet even recent work on equality-based reasoning tasks takes as its starting point the conclusion that neural networks are unable to succeed using standard architectures and general purpose learning algorithms \cite{alhama2018,weyde2019,weyde2018,kopparti2020}. Further, though tasks and contexts vary, work in both computer vision \cite{Kim:2018,Fleuret:2011,Bengio:2016} and machine reasoning \cite{raposo2017,santoro2017,santoro2018,palm2018} has presupposed that relational reasoning generally -- and sometimes equality-based reasoning specifically -- is difficult or impossible in standard network architectures.

\begin{figure}[tp]
\centering
\includegraphics[scale=0.20]{modelfigv1.pdf}
\caption{Relational reasoning tasks. Green and red mark positive and negative training examples, respectively. The sequential task (Model~2) uses only positive instances, and a model succeeds if, prompted with $\alpha$, it produces a sequence $\beta \ \alpha$ where $\beta \neq \alpha$. For the hierarchical task (Model~3), we show that a model trained on the basic task (Model~1) is effective with no additional training.}
\label{fig:tasks}
\end{figure}

Modern deep learning models have been so successful that it seems odd that they would be completely unable to learn equality-based reasoning tasks. We suspect these claims remain in the literature partly because only a narrow range of network architectures and representations were explored in the earlier debate, in part because it predated many important innovations in neural network design.
Thus we revisit the debate here, using a broader range of architectures and representations and adopting stringent criteria for generalization. In particular we explore random and pretrained representations, which have facilitated some of the extraordinary successes of modern artificial intelligence \cite{CollobertWeston:2011,Mikolov-etal:2013,pennington-socher-manning:2014:EMNLP2014,Peters-etal:2018,Devlin-etal:2019}. The use of pretrained representations to solve downstream tasks in particular is argued to be a hallmark of natural learning systems \cite{zador2019}, has been an important feature of historical models from cognitive science \cite[e.g.,][]{landauer1997,mcrae1993}, and is essential in the latest wave of state-of-the-art natural language processing models \cite{Devlin-etal:2019,Liu:2019,Radford:2019,Brown:2020}.
% While there has been criticisms of the datasets and methods used to evaluate neural models \cite{jia:2017,Kim:2018,nie:2020}

% We adopt stringent criteria for generalization and considering a broader range of representations across three models.
In our current work, we model three cases of equality-based reasoning that have featured prominently in discussions of the role of symbols in relational reasoning (\figref{fig:tasks}): (1) learning to discriminate pairs of objects that exemplify the relation \emph{same} or \emph{different}, (2) learning sequences with repeated \emph{same} elements \cite{marcus:1999}, and (3) learning to distinguish hierarchical \emph{same} and \emph{different} relations in a context with pairs of pairs exemplifying these relations \cite{Premack:1983}. Across these three models, we find strong support for their ability to learn equality relations. These results should serve to revise the conclusions of the earlier debate.

Marcus and colleagues \cite{marcus:1999,marcus:2001} showed experimentally that neural networks using feature representations cannot generalize to binary features unseen in training. We agree with this claim (and support it with a direct mathematical argument in \SIref). However, they concluded from this result that neural networks will need to have primitive symbolic operators to solve equality-based relational reasoning tasks, which is a solution that has been pursued in recent machine learning research \cite{weyde2019,weyde2018, kopparti2020}. On this point, we disagree. Our experiments show that networks without such primitives can solve a range of these tasks using the sort of random or pretrained representations that are now the norm throughout artificial intelligence research. Overall, these findings suggest that some essential aspects of symbolic reasoning can emerge from entirely data-driven, non-symbolic learning processes.

Our work here makes three contributions. First, we resolve this longstanding debate by demonstrating neural networks are able to learn  equality relations when provided with pretrained or random representations. Second, we modify the standard architecture of a recurrent neural network to allow it to learn the sequential equality task with no negative feedback. Negative evidence was dismissed as an unreasonably strong learning regime in the original debate over these issues \citep[e.g.,][]{marcus:1999a}, and we show that this learning regime is not necessary. Third, we show that a model pretrained on the simple equality task can achieve zero shot generalization to the hierarchical equality task, suggesting that pretraining might provide an account of how some organisms succeed on hard relational learning tasks. We believe these three contributions represent significant progress in our understanding of neural networks' ability to perform equality-based reasoning.  Taken together, these contributions lay the groundwork for further non-symbolic neural network models of relational reasoning and abstract thought more broadly.



\newcommand{\mysquare}{
\begin{tikzpicture}[scale=0.5]
 \node[rectangle, fill=red!100, minimum height=3mm, minimum width=3mm]{};
\end{tikzpicture}}

\newcommand{\bluetriangle}{

\begin{tikzpicture}[scale=0.25]
 \node[regular polygon,regular polygon sides=3, fill=blue!100, inner sep=2pt]{};
\end{tikzpicture}}
\newcommand{\redtriangle}{
\begin{tikzpicture}[scale=0.5]
 \node[regular polygon,regular polygon sides=3, fill=red!100, inner sep=2pt]{};
\end{tikzpicture}}
\newcommand{\bluesquare}{
\begin{tikzpicture}[scale=0.5]
 \node[regular polygon,regular polygon sides=4, fill=blue!100, minimum height=3mm, minimum width=3mm]{};
\end{tikzpicture}}
\newcommand{\redsquare}{
\begin{tikzpicture}[scale=0.5]
 \node[regular polygon,regular polygon sides=4, fill=red!100, minimum height=3mm, minimum width=3mm]{};
\end{tikzpicture}}
\newcommand{\bluepentagon}{
\begin{tikzpicture}[scale=0.5]
 \node[regular polygon,regular polygon sides=5, fill=blue!100, minimum height=3mm, minimum width=3mm]{};
\end{tikzpicture}}
\newcommand{\redpentagon}{
\begin{tikzpicture}[scale=0.5]
 \node[regular polygon,regular polygon sides=5, fill=red!100, minimum height=3mm, minimum width=3mm]{};
\end{tikzpicture}}

\newcommand{\myrectangle}{
\begin{tikzpicture}[scale=0.25]
 \node[rectangle, fill=blue!100, minimum height=3mm, minimum width=5mm]{};
\end{tikzpicture}}

\newcommand{\mypentagon}{
\begin{tikzpicture}[scale=0.5]
 \node[regular polygon,regular polygon sides=5, minimum height=4mm, fill=red!100]{};
\end{tikzpicture}}


\newcommand{\mypolygon}{
\begin{tikzpicture}[scale=0.5]
 \node[regular polygon,regular polygon sides=7, minimum height=4mm, fill=blue!100]{};
\end{tikzpicture}}

\newcommand{\minshape}{6mm}

\begin{figure*}[t!]
\setlength{\arraycolsep}{2pt}
\centering
\small
\begin{subfigure}[t]{0.1\textwidth}
 \centering
 \begin{tabular}[b]{@{} c @{}}
  \midrule
  \bluetriangle \\
  \redtriangle \\
  \bluesquare \\
  \redsquare \\
  % \bluepentagon \\
   $\vdots$ \\
  \redpentagon \\
  \bottomrule
 \end{tabular}
 \caption{}
 \label{fig:entities}
\end{subfigure}
\begin{subfigure}[t]{0.2\textwidth}
 \centering
 \renewcommand{\arraystretch}{1.18}
 $\begin{array}[b]{*{6}{c}}
   \toprule
   \bluetriangle & \redtriangle & \bluesquare & \redsquare & \bluepentagon & \redpentagon \\
   \midrule
   1 & 0 & 0 & 0& 0& 0 \\
   0 & 1 & 0 & 0& 0& 0 \\
   0 & 0 & 1 & 0& 0& 0 \\
   0 & 0 & 0 & 1& 0& 0 \\
   % 0 & 0 & 0 & 0& 1&0 \\
   \multicolumn{6}{c}{\vdots} \\
   0 & 0 & 0 & 0& 0& 1 \\
  \bottomrule
 \end{array}$
 \caption{Localist.}
 \label{fig:reps:localist}
\end{subfigure}
\begin{subfigure}[t]{0.2\textwidth}
 \centering
 \renewcommand{\arraystretch}{1.18}
 $\begin{array}[b]{*{3}{r}}
   \toprule
   \rotatebox{90}{red} & \rotatebox{90}{blue} & \rotatebox{90}{sides}\\
   \midrule
   0 & 1 & 3  \\
   1 & 0 & 3  \\
   0 & 1 & 4  \\
   1 & 0 & 4 \\
   % 0 & 1 & 5 \\
   \multicolumn{3}{c}{\vdots} \\
   1 & 0 & 5 \\
   \bottomrule
 \end{array}$
 \caption{Property.}
 \label{fig:reps:symbolic}
\end{subfigure}
\begin{subfigure}[t]{0.2\textwidth}
 \centering
 \renewcommand{\arraystretch}{1.18}
 $\begin{array}[b]{rr}
   \toprule
   d_{1} & d_{2} \\
   \midrule
   -0.4 &  0.2 \\
   0.2 &  -0.2 \\
   0.4 &  0.0 \\
   -0.2 &  -0.0 \\
   % 0.2 &  0.4 \\
   \multicolumn{2}{c}{\vdots} \\
   0.5 &  0.2 \\
   \bottomrule
 \end{array}$
 \caption{Random.}
 \label{fig:reps:random}
\end{subfigure}
\begin{subfigure}[t]{0.2\textwidth}
 \centering
 \renewcommand{\arraystretch}{1.18}
 $\begin{array}[b]{rr}
   \toprule
   d_{1} & d_{2} \\
   \midrule
   -1.3 &  0.4 \\
   -0.1 &  -1.1 \\
   -0.9 &  1.2 \\
   0.6 &  -0.6 \\
   %-0.9 &  2.1 \\
   \multicolumn{2}{c}{\vdots} \\
   1.2 &  -0.1 \\
   \bottomrule
 \end{array}$
 \caption{Pretrained.}
 \label{fig:reps:pretrained}
\end{subfigure}

\begin{subfigure}[t]{0.45\linewidth}
\centering
\begin{tikzpicture}[scale=0.6]
\begin{axis}[
 title={},
 grid=major,
axis lines=middle,
 no markers,
 every axis plot/.append style={ultra thick},
 xmin=-0.4999, xmax=0.55,
 ymin=-0.4999, ymax=0.55,
 ticks=none
]
\node[regular polygon, minimum height=6mm,regular polygon sides=3, fill=red!100,inner sep=2pt] at (axis cs:0.25644314, 0.12540874) {};
\node[regular polygon, minimum height=6mm,regular polygon sides=4, fill=blue!100] at (axis cs:0.25999054, -0.29644176) {};
\node[regular polygon, minimum height=6mm,regular polygon sides=5, fill=red!100] at (axis cs:0.049219638, 0.42767277) {};
\node[regular polygon, minimum height=6mm,regular polygon sides=6, fill=blue!100] at (axis cs:-0.061883904, 0.19825003) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=7, fill=red!100] at (axis cs:-0.37857392, 0.47314683) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=3, fill=blue!100,inner sep=2pt] at (axis cs:0.10887167, -0.26070255) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=4, fill=red!100] at (axis cs:-0.34162185, 0.050839007) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=5, fill=blue!100] at (axis cs:0.05225141, -0.4067908) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=6, fill=red!100] at (axis cs:0.46225715, 0.4129299) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=7, fill=blue!100] at (axis cs:-0.038552105, -0.38253385) {};
 %\node[regular polygon, minimum height=\minshape,regular polygon sides=3, fill=blue!100,inner sep=2pt] at (axis cs:-0.35932082, 0.17664503) {};
 %\node[regular polygon, minimum height=\minshape,regular polygon sides=3, fill=red!100,inner sep=2pt] at (axis cs: 0.23422007, -0.23213634) {};
 %\node[regular polygon, minimum height=\minshape,regular polygon sides=4, fill=blue!100] at (axis cs:0.4148342 , 0.02179685) {};
 %\node[regular polygon, minimum height=\minshape,regular polygon sides=4, fill=red!100] at (axis cs:-0.19870198, -0.02996263) {};
 %\node[regular polygon, minimum height=\minshape,regular polygon sides=5, fill=blue!100] at (axis cs: 0.23975432 , 0.40730593) {};
 %\node[regular polygon, minimum height=\minshape,regular polygon sides=5, fill=red!100] at (axis cs:0.49842966 , 0.17170991) {};
\end{axis}
\end{tikzpicture}
 \caption{The matrix in \figref{fig:reps:random}.}

 \label{fig:tsne-random}
\end{subfigure}
\begin{subfigure}[t]{0.45\linewidth}
\centering
\begin{tikzpicture}[scale=0.6]
\begin{axis}[
 title={},
 no markers,
 grid=major,
axis lines=middle,
 every axis plot/.append style={ultra thick},
 xmin=-0.9, xmax=1.8,
 ymin=-1.99, ymax=0.99,
ticks=none
]
\node[regular polygon, minimum height=6mm,regular polygon sides=3, fill=red!100,inner sep=2pt] at (axis cs:0.043389633, 0.8164206) {};
\node[regular polygon, minimum height=6mm,regular polygon sides=4, fill=blue!100] at (axis cs:-0.7053148, -1.812479) {};
\node[regular polygon, minimum height=6mm,regular polygon sides=5, fill=red!100] at (axis cs:-0.19551103, 0.4328718) {};
\node[regular polygon, minimum height=6mm,regular polygon sides=6, fill=blue!100] at (axis cs:1.6731963, -1.0493283) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=7, fill=red!100] at (axis cs:-0.19127868, -0.034780912) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=3, fill=blue!100,inner sep=2pt] at (axis cs:-0.40698338, -0.91700864) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=4, fill=red!100] at (axis cs:-0.24272422, 0.16861145) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=5, fill=blue!100] at (axis cs:0.88247913, -0.13587654) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=6, fill=red!100] at (axis cs:-0.075923756, 0.29488102) {};
\node[regular polygon, minimum height=\minshape,regular polygon sides=7, fill=blue!100] at (axis cs:2.1035564, -2.6820786) {};
 %\node[regular polygon,regular polygon sides=3, fill=blue!100,inner sep=2pt] at (axis cs:-1.2906312,  0.36732233]) {};
 %\node[regular polygon,regular polygon sides=3, fill=red!100,inner sep=2pt] at (axis cs: -0.09487347, -1.1454606) {};
 %\node[regular polygon,regular polygon sides=4, fill=blue!100] at (axis cs:-0.8569037 , 1.1844875) {};
 %\node[regular polygon,regular polygon sides=4, fill=red!100] at (axis cs: 0.56387985, -0.61090606) {};
 %\node[regular polygon,regular polygon sides=5, fill=blue!100] at (axis cs:-0.94791865, 2.0959575) {};
 %\node[regular polygon,regular polygon sides=5, fill=red!100] at (axis cs:1.2290089, -0.08361815) {};
\end{axis}
\end{tikzpicture}
 \caption{The matrix in \figref{fig:reps:pretrained}.}
 \label{fig:tsne-pretrained}
\end{subfigure}

\caption{Each matrix is a method for representing the shapes in \figref{fig:entities}, where each row is a vector representation of one shape. Localist and property are featural representations where each vector dimension encodes the value of a single, semantically interpretable property. Random and pretrained are non-featural representations where the values of properties are encoded implicitly in two dimensions. Random representations and localist representations encode only identity, whereas property representations and pretrained representations encode color and number of sides.}
\label{fig:reps}
\end{figure*}




\section{Designing theoretical models of equality learning}

We begin by discussing two critical design considerations for our models: (1) the standards for generalization by which models should be evaluated and (2) the type of representations they should use. To summarize this discussion: we select generalization tasks with fully disjoint training and test vocabularies to provide the most stringent test of generalization. Further, we adopt both randomly initialized  representations and pretrained representations for our subsequent models, and we show analytically that other kinds of representations are more limited in their capacity to make successful out-of-sample generalizations.

\subsection{Generalization}

The standard approach to training and evaluating neural networks is to choose a dataset, divide it randomly into training and assessment sets, train the system on the training set, and then use its performance on the assessment set as a proxy for its capacity to generalize to new data.

The standard approach is fine for many purposes, but it raises concerns in a context in which we are trying to determine whether a network has truly acquired a global solution to a target function. In particular, where there is any kind of overlap between the training and assessment vocabularies (primitive elements), we can't rule out that the network might be primarily taking advantage of idiosyncrasies in the underlying dataset to effectively cheat -- to memorize aspects of the training set and learn a local approximation of the target function that happens to provide traction during assessment.

To address this issue, \update{we follow \cite{marcus:1999} in proposing} that networks must be evaluated on assessment sets that are completely disjoint in every respect from the train set, all the way down to the entities involved. For example, below, we train on pairs $(a, a)$ and $(a, b)$, where $a$ and $b$ are representations from a train vocabulary $V_{T}$. At test time, we create a new assessment vocabulary $V_{A}$, derive equality and inequality pairs $(\alpha, \alpha)$ and $(\alpha, \beta)$ from that vocabulary, and assess the trained network on these new examples. In adopting these methods, we get a clear picture of the system's capacity to generalize, and we can safely say that its performance during assessment is a window into whether a global solution to identity has been learned. This is a very challenging setting for any machine learning model.
% For the sequential same--different task, we will see that it even requires us to depart from usual model formulations.

\subsection{Representations}\label{sec:representations}

% \updatea{
Essentially all modern machine learning models represent objects using vectors of real numbers. However, there are important differences in how these vectors are used to encode the properties of objects. The method of representation impacts whether there is a natural notion of similarity between entities and the ability of models to generalize to examples unseen in training. These two attributes are deeply related; if there is a natural notion of similarity between vector representations, then models can generalize to inputs with representations that are similar to those seen in training.

We characterize two broad approaches to such property encoding -- which we call \tech{featural representations} and \tech{non-featural representations} -- and argue that the differences between them have not been given sufficient attention in the debate about the ability of neural networks to perform relational reasoning. We acknowledge that a dimension of any vector representation is a ``feature'' but we adopt a usage that is common in cognitive science, namely that a feature is an interpretable semantic primitive.\footnote{The term \tech{distributed representations} is used to refer jointly to what we call property representations, random representations, and pretrained representations. We opted not to use this term because it does not seperate property representations from random and pretrained representations, which is the relevant division here. Distributed representations are often contrasted in the neural network literature with \tech{local} or \tech{localist} representations; as discussed below, here we define these terms specifically to refer to representations whose features correspond to specific entities.}

We ground our discussion in a hypothetical universe of blocks which vary by shape and color. \Figref{fig:entities} is a partial view of them, and \figref{fig:reps:localist}--\figref{fig:reps:pretrained} present four different ways of encoding the properties of these objects in vectors.

\subsubsection{Featural Representations}

The defining characteristic of \tech{featural} vector representations is that each dimension encodes the value of a single, semantically intepretable property. The properties can be binary, integer-valued, or real-valued.

We use the term \tech{localist} for the special case of featural representations in which only objects are represented and there is a feature corresponding to each object. In \figref{fig:reps:localist}, each column represents the property of being an object, and every object is represented as a vector that has a single unit with value 1. There is no shared structure across objects; all are equally (un)related to each other as far as the model is concerned.

We will refer to featural representations that are not localist as \tech{property representations}. Here, column dimensions encode specific, meaningful properties of objects. In our example, we can represent the properties of being red and being blue with two different binary features, and the property of having a certain number of sides as a single integer feature, as in \figref{fig:reps:symbolic}. Unlike with localist representations, objects in this space can have complex relationships to each other, as encoded in the shared structure given by the columns.

Featural representations -- both localist and property -- have the appealing property that they are easy for researchers to interpret because of the tight correspondence between column dimensions and properties. However, this transparency actually inhibits neural networks from discovering general solutions. Instead, such models work far better with representations that have property values implicitly encoded in the abstract structure of the vector space. We demonstrate this result analytically in \SIref\ for the case of binary features. The core insight is that networks cannot learn anything about column dimensions that are not represented in their training data; whatever weights are associated with those dimensions are unchanged by the learning process, so predictions about those dimensions remain random at test time. A developmental perspective suggests another reason to avoid binary featural representations, namely that this is not an accurate account of how perceptual inputs are represented in the brain.

Recent work in machine learning \cite{weyde2019,weyde2018,kopparti2020} attempts to overcome this analytic limitation of binary featural representations by modifying standard neural architectures to have symbolic primitives or changing network weight priors. In our work, we instead opt for non-featural representations, which do not have this analytic limitation and are the norm in state-of-the-art artificial intelligence models. There is no need to introduce symbolic primitives or modify network weight priors when non-featural representations are used.

\subsubsection{Non-Featural Representations}

A \tech{non-featural representation} is a vector that encodes property values implicitly across many dimensions. Perhaps the simplest non-featural representations are \tech{completely random} vectors, as in \figref{fig:reps:random}. Random representations can be seen as the non-featural counterpart to localist representations. In both of these representation schemes, all the objects are equally (un)related to each other, since column-wise patterns are unlikely in random representations and, to the extent that they are present, they exist completely by chance. However, in random representations, all the column dimensions can contribute meaningfully to identifying objects, whereas a localist representation has only one vector unit that determines the identity of any given object.

Random representations are a starting point that encodes object identity, but we can \tech{pretrain} these representations via a learning process, imbuing them with rich structure that implicitly encodes property values across many dimensions. \Figref{fig:reps:pretrained} provides a simple example. This matrix is the results of pretraining the representations in \figref{fig:reps:random} on the task of predicting whether the object is blue, whether the object is red, and the number of sides the object has. (\Appref{app:pretraining} provides technical details on our pretraining approach.) Superficially, the two matrices look equally random, but the random representations in \figref{fig:tsne-random} have no such structure, while the pretrained representations in \figref{fig:tsne-pretrained} do: there is a line that separates blue and red objects.

Pretraining need not be restricted to input representations; all the parameters of a model can be pretrained, offering the possibility that networks might be used as modular components to solve more complex tasks. We realize this possibility with our third experiment, where a model pretrained on a simple equality is used as a modular component to compute hierarchical equality.
% In this setting, a small amount of task-specific training (often called ``fine-tuning'') might suffice.

% \subsubsection{The Superiority of Non-Featural Representations}

% We argue that non-featural representations are superior to featural ones for two reasons: (1) non-featural representations are more biologically plausible than feature representations; and (3) featural representations can seriously inhibit the ability of neural networks to generalize to unseen examples.

% From a biological perspective, we might want the representations we feed our artificial neural networks to be similar to perceptual neural representations in biological neural networks. We believe it is obvious that biological neural representations are such that property values are not explicitly encoded or localized to the activity of a single neuron. For example, when a human looks at a red square block that is red, the property of being red is not encoded in the activity of a single neuron in the vision system, but instead is encoded in the activations of many neurons. This is a property that is shared with pretrained non-featural representations, but not with feature representations, where property values are explicit and localized.



% \subsubsection{On the Symbolic/Non-Symbolic Distinction}


% \subsection{The current paper}

%\begin{figure*}[tp]
%\centering
%\begin{subfigure}[t]{0.48\linewidth}
% \includegraphics[width=1\linewidth]{../fig/equality-train_size-embed_dim-hidden_dim=100.pdf}
% \vspace{-4mm}
% \caption{Results for a model in which the hidden layer has dimensionality $100$. The lines correspond to different dimensions for the entities $a$ and $b$ in the input pairs $(a, b)$. The best model passes 90\% accuracy with 1,000 training examples and reaches (near) perfection by 1,250.}
% \label{fig:equality--smallresults}
%\end{subfigure}
%\hfill
%\begin{subfigure}[t]{0.48\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{../fig/equality-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
% \caption{\update{Results where random representations are grounded in a number of property domains via pretraining learning tasks. The `no pretraining' model is the best of the models at left (10-dimensional embeddings, 100-dimensional hidden layers), repeated to facilitate comparisons. The pretraining models use those same dimensionalities.}}
% \label{fig:basic-equality-pretrain}
%\end{subfigure}
%\caption{Same--different results with and without pretraining.}
%\end{figure*}

\section{Model 1: Same--different relations}\label{sec:equality}

First, we investigate whether a supervised single layer feed-forward neural network can learn the equality relation in the strict setting we describe above where train and test vocabulary are disjoint. The input is a pair of vectors $(a, b)$ which correspond to the two stimulus objects. These vectors are non-featural representations that do not have features encoding properties of the objects or their identity. During training, this model is presented with positive and negative labeled examples. During testing, this model is tasked with categorizing inputs unseen during training. It is straightforward to show that a network like this is capable of learning equality as we have defined it. Indeed, in our \SIref, we provide an analytic solution to the equality relation using this neural model.
% \Appref{app:equality-solution} provides

This result shows that equality in our sense is learnable in principle, but it doesn't resolve the question of whether networks can find this kind of solution given finite training data. To address this issue, we train networks on a stream of pairs of random vectors. Half of these are identity pairs $(a, a)$, labeled with $1$, and half are non-identity pairs $(a, b)$, labeled with $0$. Trained networks are assessed on the same kind of balanced dataset, with vectors that were never seen in training so that, as discussed earlier, we get a clear picture of whether they have found a generalizable solution.

\subsection{Results}

\Figref{fig:basic-equality-pretrain} shows our results. The representations used in these experiments are random representations that were pretrained using a linear classifier for 0, 3, 5, or 10 different binary feature discrimination tasks. For example, following \figref{fig:reps}, a three-task model might be trained to encode the binary properties of being blue, having four sides, and being red. For all representations, this neural model reached above-chance performance almost immediately, but required upwards of 1,000 examples to achieve near perfect accuracy.   Interestingly, we observed a clear speed-up, with more pretraining tasks resulting in the largest gains. It seems that, by grounding our representations in ``property domains'' (as represented by the different task dimensions), we imbue them with implicit structure that makes learning easier.

%\Figref{fig:equality--smallresults} presents typical results. This is for the case where the hidden layer dimensionality is $100$, and we plot results for different embedding dimensionalities $m$ and different amounts of training data. The picture is comparable with hidden dimensionalities at 10, 25, and 50, but those models require more training data to reach (near) perfect performance (\appref{app:model1-results}).

%While all the networks in \figref{fig:equality--smallresults} reach above-chance performance almost immediately, they require upwards of 1,000 examples to truly solve these tasks. We additionally ran the above experiments using random representations that were pretrained using a linear classifier for 0, 3, 5, or 10 different feature discrimination tasks. For example, following \figref{fig:reps}, a three-task model might be trained to encode the properties of color, number of sides, and size.

%\Figref{fig:basic-equality-pretrain} summarizes the results of these experiments for 10-dimensional embeddings and 100-dimensional hidden representations, as this seems to be the network that learns the fastest with random inputs. Interestingly, we see a clear speed-up, with more pretraining tasks resulting in the largest gains; by grounding our representations in ``property domains'' (as represented by the different task dimensions), we imbue them with implicit structure that makes learning easier.

\subsection{Discussion}

Our assessment pairs have nothing in common with the training pairs except insofar as both involve vectors of real numbers of the same dimensionality. During training, the network is told (via labels) which pairs are equality pairs and which are not, but the pairs themselves contain no information about equality per se. It thus seems fair to us to say that these networks have learned equality -- or at least how to simulate that relation with near perfect accuracy. Further, the use of representations that are structured by pretraining results in faster learning.



%\begin{figure*}[tp]
%\centering
%\begin{subfigure}[t]{0.48\textwidth}
% \centering
% \includegraphics[width=1\linewidth]{../fig/fuzzy-lm-vocab20-train_size-embed_dim-hidden_dim=100.pdf}
% \caption{Results for a model in which each $h_{t}$ in Eq.~\eg{eq:lstm-recur} has dimensionality 100. The lines correspond to the dimensionality of the input representations ($x_t$ in Eq.~\eg{eq:lstm-recur}). All the training examples are presented at once over multiple epochs.}
% \label{fig:fuzzy-lm-results}
%\end{subfigure}
%\hfill
%\begin{subfigure}[t]{0.48\textwidth}
% \centering
% \includegraphics[width=1\linewidth]{../fig/fuzzy-lm-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
% \caption{Results where random representations are grounded via pretraining learning tasks. Simulations adopt the best-performing settings from the no-pretraining condition (2-dimensional embeddings, 100-dimensional hidden representations).}
% \label{fig:fuzzy-lm-pretrain-results}
%\end{subfigure}
%\caption{Sequential same--different results with and without pretraining.}
%\end{figure*}


\begin{figure*}[tp]
\centering
\begin{subfigure}[t]{0.48\linewidth}
 \centering
 \includegraphics[width=1\linewidth]{../fig/equality-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
 \caption{Results for single layer feed-forward neural networks trained on our simple equality task. The 'no pretraining' model is provided random representations and the 'k-task pretraining' models are provided random representations are grounded in $k$ binary property domains via pretraining learning tasks. }
 \label{fig:basic-equality-pretrain}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
 \centering
 \includegraphics[width=1\linewidth]{../fig/fuzzy-lm-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
 \caption{Results for LSTM recursive neural networks trained on our sequential equality task. The 'no pretraining' model is provided random representations and the 'k-task pretraining' models are provided random representations are grounded in $k$ binary property domains via pretraining learning tasks.  All the training examples are presented at once over multiple epochs}
 \label{fig:fuzzy-lm-pretrain-results}
\end{subfigure}
 \caption{Results for (a) the simple equality task and (b) the sequential equality task.}
\end{figure*}





\section{Model 2: Sequential same--different (ABA task)}\label{sec:lms}

Our first model is simple and successfully learns equality. However, this model is supervised with both positive and negative evidence. In the initial debate around these issues, supervision with negative evidence was dismissed as an unreasonably strong learning regime \citep[e.g.,][]{marcus:1999a}. While this argument likely holds true for language learning \citep[in which supervision is generally agreed not to be binary or direct;][]{brown:1970,chouinard2003}, it is not necessarily true for learning more generally. Nevertheless, learning of sequential rules without negative feedback is possible for infants \cite{marcus:1999,rabagliati:2019}. In experiments of this type, infants are presented with a set of positive examples. Our next model explores whether neural network models can learn this task in a challenging regime with no negative supervision.

To explore learning with only positive instances, we use a neural LSTM language model, a recursive network with the ability to selectively forget and remember information \cite{hochreiter:1997}. Language models are sequential: at each timestep, they predict an output given their predictions about the preceding timesteps. As typically formulated, the prediction function is just a classifier: at each timestep, it predicts a probability distribution over the entire vocabulary of options, and the item with the highest probability is chosen as a symbolic output. This output becomes the input at the next timestep, and the process continues.

This formulation will not work in situations in which we want to make predictions about test items with an entirely disjoint vocabulary from the training sample. The classifier function will get no feedback about these out-of-vocabulary items during training, and so it will never predict them during testing. To address this issue, we reformulate the prediction function. Our proposal is to have the model predict output vector representations -- instead of discrete vocabulary items -- at each timestep. During training, the model is trained to minimize the distance between these output predictions and the representations of the actual output entities. During assessment, we take the prediction to be the item in the entire vocabulary (training and assessment) whose representation is closest to the predicted vector (in terms of Euclidean distance). This fuzzy approach to prediction creates enough space for the model to predict sequences from an entirely new vocabulary. Our \SIref\
% \Appref{sec:analyticlm}
provide an analytic solution to the ABA task using this model.

To see how well the model performs in practice, we trained networks on sequences \texttt{<s> a b a </s>}, where $\texttt{b} \neq \texttt{a}$. We show the network every such sequence during training, from an underlying vocabulary of 20 items (creating a total of 380 examples). To assess how well the model learns this pattern, we seed it with \texttt{<s> x} where \texttt{x} is an item from a disjoint vocabulary from that seen in training, and we say that a prediction is accurate if the model continues with \texttt{y x </s>}, where \texttt{y} is any character (from the training or assessment vocabulary) except \texttt{x}.

\subsection{Results}

\Figref{fig:fuzzy-lm-pretrain-results} shows our results. Unlike for the previous equality experiment, we found that we had to allow the model to experience multiple epochs of training on the same set in order to succeed and tens of thousands of training examples were necessary. We considered a range of representations (as in Model~1); the model was again successful with all representations, but in this experiment pretraining representations did not increase performance.


\subsection{Discussion}

These sequential models are given no negative examples and they must predict into a totally new vocabulary. Despite these challenges, they succeed at learning the underlying patterns in our data. On the other hand, the learning process is slow and data-intensive. We hypothesized that grounding representations in property domains via pretraining might lead to noticeable speed-ups, as it did in for our simple same-different task, but we did not see this effect in practice. We speculate that there may be model variants that reduce these demands, given that learning is in principle possible in this architecture, but we leave them to future work.

\section{Model 3: Hierarchical same--different relations}\label{sec:premack}

Given the strong results found for simple equality relations, we can ask whether more challenging equality problems are also learnable in our setting. The hierarchical equality task used by Premack \cite{Premack:1983} is an interesting test case: given a pair of pairs $((a,b), (c,d))$, the label is $1$ if $(a = b) = (c = d)$, else $0$. Premack suggested that the ability exemplified by this task -- reasoning about hierarchical \emph{same} and \emph{different} relations -- could represent a form of symbolic abstraction uniquely enabled by language. Given the non-symbolic nature of our models, our simulations provide a test of this hypothesis, though we should look critically at their ability to find good solutions with reasonable amounts of training data.


\begin{figure*}[tp]
\centering
\begin{subfigure}[t]{0.48\linewidth}
 \centering
 \includegraphics[width=1\linewidth]{../fig/flatpremack-h2-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
 \caption{Results for two layer feed-forward neural networks trained on our hierarchical equality task. The 'no pretraining' model is provided random representations and the 'k-task pretraining' models are provided random representations are grounded in $k$ binary property domains via pretraining learning tasks. }
 \label{fig:premack-h2-pretrain}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=1\linewidth]{../fig/input-as-output-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
\caption{Results for simple equality networks applied to the hierarchical equality task. The 'no pretraining' model is provided random representations and the 'k-task pretraining' models are provided random representations are grounded in $k$ binary property domains via pretraining learning tasks.
%Even with no additional training instances for this task, all models achieve greater than chance accuracy, and even modest amounts of additional training on the task lead to excellent performance.
}
\label{fig:premack-pretraining-results}
\end{subfigure}
 \caption{Results for (a) hierachical sequential equality task without pretraining on simple equality and (b) with pretraining on simple equality.}
\end{figure*}
%\begin{figure*}[tp]
%\centering
%\begin{subfigure}[t]{0.48\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{../fig/flatpremack-h2-train_size-embed_dim-hidden_dim=100.pdf}
% \caption{Results for a network with two 100-dimensional hidden layers. Nearly all the networks solve the task, but they require very large training sets to do so.}
% \label{fig:premack-h2-flat-results}
%\end{subfigure}
%\hfill
%\begin{subfigure}[t]{0.48\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{../fig/flatpremack-h2-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
% \caption{Results from pre-trained networks, adopting the best of the models at left (10-dimensional embeddings, 100-dimensional hidden representations).}
% \label{fig:premack-h2-pretrain}
%\end{subfigure}
%\caption{Hierarchical same--different results with and without pretraining.}
%\end{figure*}

We can approach this task using the same model and methods as we used for equality, with the relatively minor change of providing the network four vector representations instead of two. We found that single layered feed-forward neural networks required nearly 100,000 training examples to solve this task.  We hypothesized that a single layer network might be suboptimal here. This task is intuitively hierarchical: if one works out the equality labels for each of the two pairs, then the further classification decision can be done entirely on that basis. Our current neural network might be too shallow to find this kind of decomposition. To address this issue, we use a two layer feed forward network.

\subsection{Results without pretraining}

\Figref{fig:premack-h2-pretrain} shows our results. We again considered a range of representations and again the network succeeded across this range, with pretraining increasing performance as in Model~1. The network required more than 20,000 training instances to reach top performance, and upwards of 10,000 examples with pretrained representations.

This amount of training data is vastly more data than human participants get in similar experiments, which typically involve short exposures in the range of dozens to hundreds of examples \cite[e.g.,][]{marcus:1999,endress2005}. Thus, it is worth asking whether there are other solutions that would be more data efficient and more in line with human capabilities. We next seek to further capitalize on the hierarchical nature of this task by defining a modular pretraining regime in which previously learned capabilities are recruited for new tasks.

\subsection{The critical role of experience}\label{sec:modular}

Our successful results training neural networks on simple equality suggested another strategy for solving the hierarchical equality task. Rather than requiring our networks to find solutions from scratch, we pretrained them on basic equality tasks and then used those parameters as a starting point for learning hierarchical equality. This set of simulations was conceptually similar to our previous experiments with pretrained input representations, but now we pretrained an entire subpart of the model, rather than just input representations. This approach parallels the experimental paradigm used by Thompson et al. \cite{thompson:1997}, in which chimpanzees that received pre-training on a basic equality (same/different judgment) task -- but not naive champanzees -- succeed in a hierarchical equality task.

The hierarchical equality task requires computing the equality relation three times: compute whether each pair of inputs are equal and then compute whether the truth-valued outputs of these first two computations are equal. We thus used the same network pretrained on basic equality to perform all three equality computations.

%\begin{figure*}[tp]
%\centering
%\begin{subfigure}[t]{0.48\textwidth}
%\includegraphics[width=1\linewidth]{../fig/input-as-output-train_size-embed_dim-hidden_dim=0.pdf}
%\caption{Basic equality networks applied to the hierarchical equality task. Each line represents a different embedding dimensionality, which is constrained in this model to match the hidden dimensionality. Even with no additional training instances for this task, all models achieve greater than chance accuracy, and even modest amounts of additional training on the task lead to excellent performance.
%}
%\label{fig:premack-pretraining-results}
%\end{subfigure}
%\hfill
%\begin{subfigure}[t]{0.48\textwidth}
% \centering
% \includegraphics[width=1\linewidth]{../fig/input-as-output-zero-shot.pdf}
% \caption{\update{The results from the plot at left where the x-axis is 0, that is, where we are testing zero-shot generalization from the pretrained equality network to the hierarchical setting. All the models perform well above chance in this setting, with the 50-dimensional version achieving a mean of 67\% accuracy.}}
% \label{fig:premack-pretraining-zero-shot-results}
%\end{subfigure}
%\caption{Modular network results for the hierarchical same--different task.}
%\label{fig:modular-results}
%\end{figure*}

% \subsubsection{Results and discussion}

\Figref{fig:premack-pretraining-results} shows our results. All the models have above chance performance after being trained only on the simple equality task -- that is, they achieve zero-shot generalization to the hierarchical task and within two thousand examples, the models achieve near perfect accuracy. It is remarkable that a model trained only on equality between entities is able to get traction on a problem that requires determining whether equality holds between the truth values encoded in two learned representations. Pretrained representations did not increase performance.

%It might be possible to effectively combine network pretraining with input pretraining as in the previous experiments. An initial exploration of this idea is presented in \appref{app:double-pretrain}. While we have not yet found a way to use this combination of pretraining regimes to improve over \figref{fig:modular-results}, we are optimistic about such combinations for future work.

\section{General Discussion}

% Further, an important line of work on relational reasoning in neural network architectures sidesteps the issue of whether relational
 % the issue of whether equality relations can be induced from data by creating networks whose architecture is explicitly relational \citep{raposo2017,santoro2018,santoro2019,palm2019}.

Equality is a key case study for understanding the origins of human relational reasoning. This case study has been puzzling for symbolic accounts of reasoning because such accounts do not provide a compelling explanation for why some equality tasks are so easy to learn and others are so hard. In addition, evidence of graded learning and generalization in non-human species suggests that a gradual learning account might provide more traction in explaining the empirical data \cite{wasserman:2017}. Inspired by this work, we revisited a long-standing debate about whether neural network models can learn equality relations from data \cite{alhama:2019}.

Our work here makes three contributions to this debate. First, we show that non-featural representations - both random and pretrained -- allow standard neural networks to learn simple, sequential, and hierarchical equality tasks. Both the research that originated this debate \cite{marcus:1999,marcus:2001,dienes:1999,seidenberg:1999a,seidenberg:1999b,elman:1999,negishi:1999} and more recent work \cite{weyde2019, weyde2018, kopparti2020, alhama2018} only involve experiments where featural representations are used, and we suggest that this choice led directly to the conclusions of this body of work. Second, we show that neural networks can achieve high test accuracy on the sequential equality task with no negative feedback, suggesting that a negative feedback learning regime is not critical for learning equality. Finally, we show that a neural network trained only on simple equality can generalize to hierarchical equality, even in a ``zero-shot'' evaluation. Although pretrained representations sometimes led to faster learning, they were not a necessary component for models to succeed, and success was possible even using random representations.

% We further supported these results with analytic insights into why networks are able to find solutions to these tasks.

In some settings, our current models require many more training instances than humans seem to need. However, our pretraining approach suggests a path forward: by using pretrained models as modular components, we can get traction on challenging tasks without any training specifically for those tasks. In some cases, even a small amount of additional training can make a substantial difference. Perhaps pretrained components of this type could serve as the basis for more complex cognitive abilities more generally \cite{frank2008,heyes2018}. Other computational work on relational reasoning in cognitive science has developed hybrid architectures that do not explicitly encode symbolic equality but incorporate other symbolic structures \cite[e.g.,][]{hummel:2003}; integrating functions learned from data could be an interesting direction for future work with these architectures. Similarly, other work on relational reasoning in artificial intelligence has used networks with explicitly relational architectures -- such architectures could be interesting to explore in the context of equality reasoning \citep{raposo2017,santoro2017,santoro2018,palm2018}.

One further implication of our pretraining findings is that it should be possible to scaffold non-human animals' performance in complex, hierarchical equality tasks via training on simpler ones. Indeed, \cite{smirnova2015} show just this result in crows, consistent with our findings. Although we do not discount the potential role of linguistic labels in informing adult humans' expertise in such tasks \cite{gentner2003}, pretraining also provides a potential account of how infants and young children might succeed in a range of equality reasoning tasks without access to specific linguistic symbols like ``same'' \cite{walker:2016,ferry:2015,hochmann2016}.

More broadly still, our work suggests a possible way forward in understanding the acquisition of logical semantics. Graded logical functions like those our models learned here could form the foundation for a semantics of words like ``same'' \cite{potts2019}. Such an option is appealing because it escapes from the circularity of defining the semantics of linguistic symbols as originating in a mental primitive {\sc same}. A semantics for ``same'' requires defining its inputs and outputs as well as how it composes with other symbols. The assertion that there is a primitive identity computation does not specify the format of these inputs and outputs or these composition rules; it further fails to explain the flexibility that allows us to call two Toyota Corollas ``the same'' but two twin sisters ``different.'' In contrast, the kinds of networks we propose here could in principle be conditioned contextually to provide flexible, context-sensitive interpretation of logical meaning. Such a contextually-conditioned semantics could be applied in both cases of sameness and graded similarity \cite{medin1993}, holding the promise of unifying models of identity and similarity.   % MCF: could we cite some of Ruben's work or the natural language inference work here maybe? or is there something more appropriate?

Earlier debates about the nature of equality computations centered around the question of whether models included symbolic elements. We believe ours do not; but it is of course possible to quibble with this judgment. For example, since the supervisory signal used in Models 1 and 3 is generated based on a symbolic rule, perhaps that makes these models symbolic under some definition. (Of course, the same argument could be applied to the supervision signal that is provided to crows, baboons, and human children in some tasks). We view this kind of argument as terminological, rather than substantive. In the end, our goal is an explicit learning theory for relational reasoning. Our hope is that the work described here takes a first step in this direction.

% The claim that certain forms of representation are equivalent to including symbolic primitives is untroubling if these representations enhance the generalization skills of neural models; some class of neural models might be labeled as symbolic, but that does not nullify the fact that they are successful models of graded relational learning and distinct from uncontroversially symbolic models \cite[e.g.,][]{frank:2011}.


% BACK TO SYMBOLIC, LANGUAGE, SEMANTICS

% the debate has been whether there is a symbol. We view this as the least important issue. A single ``same" symbol cannot account for the vast range of behaviors that we want to explain -- how are two toyota corollas the same, while two people are not?
% To be an explanatory account, a symbol needs a semantics - what are the inputs, outputs, how does it compose. Without these, there is no account of behavior.

% same isn't a relation, it's a set of possible relations


% you can think of it this way: our networks are a semantics for the symbol

% This is maybe unfamiliar, because you might think you'd do this in a logical language or have an axiom system, but our networks are perfect and so you'd never notice. MARR cite



% We report results with and without pretraining. While \citeauthor{marcus:2001} might consider our pretraining method to be symbolic, we believe it would be difficult to argue that random distributed representations are symbolic, as they do not encode any properties directly. We additionally believe that the symbolic/non-symbolic distinction does not provide an explanation for the state of the comparative and developmental literature on relational learning, as we will argue in \secref{sec:?}.}

% SYMBOLIC VS. NON-Symbolic
% - this distinction doesn't helps
% - neural models clearly different
% - supervision unclear
% - not helpful (localist could be symbolic & unstructured)
%
% Since the initial debate, neural networks have become

%Finally, we note that, while the pretrained representations in \figref{fig:reps:pretrained} encode property values via linear structures, this is only because we used linear classifiers in our pretraining tasks. There are a practically limitless number of different ways to encode property values via abstract structures.

 % By contrast, the literature on relational learning in neural networks is currently limited to feature representations, and does not consider any non-featural representations where property values are encoded implicitly with abstract structures. For example, all of the experiments reviewed in \cite{alhama:2019} use feature representations, meaning that each vector unit explicitly contains the value of a property. Their negative conclusions about relational learning in neural networks hold only for networks using such representations.

\section{Materials and Methods}
The code and data used to run the experiments in this paper are publicly available at: URL withheld for review%https://github.com/atticusg/NeuralRelationalReasoning

The simple equality model takes in input vectors, $a,b$, and uses a single linear transformation followed by a non-linearity to create a hidden representation $h$
that is then used to create a probability distribution, $y$, over the two classes.
\begin{align}
  h &= \ReLU([a;b]W_{xh} + b_{h})  & y = \softmax(hW_{hy} + b_{y}) \label{eq:h2y}
\end{align}

The sequential equality model takes in a sequence of input vectors, $x_1,x_2,\dots$, and uses an LSTM cell to create a hidden representation $h_t$ at each timestep $t$ that is
that is then linearly projected into the input vector space providing a prediction for that timestep, $y_t$.
\begin{align}
  h_{t} &= \LSTM(x_{t}, h_{t-1})  & y_{t} = h_{t}W + b\label{eq:lstm-predict}
\end{align}

The first hierarchical equality model takes in input vectors, $a,b,c,d$, and applies a linear transformation followed by a non-linearity to create a hidden representation $h_1$ and then
applies these two steps once more to create second hidden representation $h_2$ that is then used to create a probability distribution, $y$, over the two classes.
\begin{align}
  h_{1} &= \ReLU([a;b;c;d]W_{xh} + b_{h_{1}}) \label{eq:x2h1}\\
  h_{2} &= \ReLU(h_{1}W_{hh} + b_{h_{2}}) & \hspace{-10pt}y = \softmax(h_{2}W_{hy} + b_{y}) \label{eq:h2y2}
\end{align}

The second hierarchical equality model takes in input vectors, $a,b,c,d$, and applies the simple equality model from equations \eg{eq:h2y} to the pairs $(a,b)$
and $(c,d)$ to produce hidden representations $h_1$ and $h_2$. Then the simple equality model is applied once again to the pair $(h_1,h_2)$ to produce a final hidden representation $h_3$
that is used to create a probability distribution, $y$, over the two classes.
\begin{align}
  h_1 &= \ReLU([a;b]W_{xh} + b_{h}) & h_2 = \ReLU([c;d]W_{xh} + b_{h})\\
  h_3 &= \ReLU([h_1;h_2]W_{xh} + b_{h}) & y = \softmax(h_3W_{hy} + b_{y}) \label{eq:prey}\
\end{align}

The parameters for the simple and hierarchical equality models are learned using back propagation with a cross entropy objective function defined as follows, for a set of $N$ examples and $K$ classes:
%
\begin{equation}\label{eq:crossent}
\max(\theta)
\quad
\frac{1}{N}
\sum_{i=1}^{N}
\sum_{k=1}^{K}
y^{i,k} \log(h_{\theta}(i)^{k})
\end{equation}
%
where $\theta$ abbreviates the model parameters, $y^{i,k}$ is the actual label for example $i$ and class $k$, and $h_{\theta}(i)^{k}$ is the corresponding prediction.

The parameters for the sequential model are learned using back propagation with a squared mean error objective function defined as follows:
%
\begin{equation}
\max(\theta)
\quad
-\frac{1}{N}
\sum_{i=1}^{N}
\sum_{t=1}^{T_{i}}
\left\| h_{\theta}\left(x^{i, 0:{t-1}}\right) - x^{i,t} \right\|^{2}
\end{equation}
%
for $N$ examples. Here, $T_{i}$ is the length of example $i$. As before, $\theta$ abbreviates the parameters of the model. We use $h_{\theta}(x^{i, 0:{t-1}})$ for the vector predicted by the model for example~$i$ at timestep~$t$, which is compared to the actual vector at timestep $t$ via squared Euclidean distance.

Hyperparameter searches and implementation details can be found in \appref{app:optimization}.

%\section*{Acknowledgements}

%\input{acknowledgements}

\bibliographystyle{acl_natbib}
\bibliography{relational-learning-bib}

\section*{Appendices}
\appendix

%\input{pre-pnas-supp}

%\section{Model 1 Supplementary Methods}

%The input to our model is a pair of vectors $(a, b)$, each of dimension $m$, which correspond to the two stimulus objects. These vectors are non-featural representations that do not have features encoding properties of the objects or their identity. These are concatenated to form a single vector $[a;b]$ of dimension $2m$, which is the simplest way of merging the two representations to form a single input.

%This representation is multiplied by a matrix of weights $W_{xh}$ of dimension $2m \times n$ and a bias vector $b_{h}$ of dimension $n$ is added to this result, where $n$ is the hidden layer dimensionality. These two steps create a linear projection of the input representation, and the bias term is the value of this linear projection when the input representation is the zero vector. Then, the non-linear activation function $\ReLU$ ($\ReLU(x) = \max(0, x)$) is applied element-wise to this linear projection. This non-linearity is what gives the neural model more expressive power than a logistic regression \citep{Cybenko:1989,Hornik:Stinchcombe:White:1989}. The result is the hidden representation $h$.

%The hidden representation is the input to the classification layer: $h$ is multiplied by a second matrix of weights $W_{hy}$, dimension $n \times 2$, and a bias term $b_{y}$ (dimension 2) is added to this. This second bias term encodes the probabilities of each class when the hidden representation is 0. The result is fed through the softmax activation function: $\softmax(x)_{i} = \frac{\exp{x_{i}}}{\sum_{j} \exp{x_{j}}}$. This creates a probability distribution over the classes (positive and negative). For a given input, the model computes this probability distribution and the input is categorized as the class with the higher probability.

%During training, this model is presented with positive and negative labeled examples and the parameters $W_{xh}$, $W_{hy}$, $b_{y}$, and $b_{h}$ are learned using back propagation with a cross entropy function. \update{This function is defined as follows, for a set of $N$ examples and $K$ classes:
%
%\begin{equation}
%\max(\theta)
%\quad
%\frac{1}{N}
%\sum_{i=1}^{N}
%\sum_{k=1}^{K}
%y^{i,k} \log(h_{\theta}(i)^{k})
%\end{equation}
%
%where $\theta$ abbreviates the model parameters ($W_{xh}$, $W_{hy}$, $b_{y}$, $b_{h}$), $y^{i,k}$ is the actual label for example $i$ and class $k$, and $h_{\theta}(i)^{k}$ is the corresponding prediction.}


%\section{Model 2 Supplementary Methods}

%The input to this model is a sequence of vectors $x_1, x_2, x_3, \dots$, each of dimension $m$, which correspond to a sequence of stimulus objects. These vectors are, again, non-featural representations that do not have features encoding properties of the objects or their identity.


%At each timestep $t$, the input vector $x_t$ is fed into the $\LSTM$ cell along with the previous hidden representation $h_{t-1}$. The defining feature of an $\LSTM$ is the ability to decide whether to store information from the current input, $x_t$, and whether to remember or forget the information from the previous timestep $h_{t-t}$. The output of the $\LSTM$ cell is the hidden representation for the current time step $h_t$. The dimension of the hidden representations is $n$. The hidden representation is multiplied by a matrix $W$ with dimensionality $n \times m$ to produce $y_t$. This result, $y_t$, is a linear projection of the hidden representation into the input vector space, which is necessary because $y_t$ is a prediction of what the next input, $x_{t+1}$, will be.

% \update{
%The objective function is as follows:
%
%\begin{equation}
%\max(\theta)
%\quad
%-\frac{1}{N}
%\sum_{i=1}^{N}
%\sum_{t=1}^{T_{i}}
%\left\| h_{\theta}\left(x^{i, 0:{t-1}}\right) - x^{i,t} \right\|^{2}
%\end{equation}
%
%for $N$ examples. Here, $T_{i}$ is the length of example $i$. As before, $\theta$ abbreviates the parameters of the model as specified in \dasheg{eq:lstm-recur}{eq:lstm-predict}. We use $h_{\theta}(x^{i, 0:{t-1}})$ for the vector predicted by the model for example~$i$ at timestep~$t$, which is compared to the actual vector at timestep $t$ via squared Euclidean distance (i.e., the mean squared error).
% As noted above, this is an unusual formulation for a language model. The usual version essentially treats every timestep as involving a classification decision, with a cross-entropy loss. We cannot adopt this because of our goal of using unseen vocabulary items at test time. % MCF: seemed redundant

%% Comment out or remove this line before generating final copy for submission; this will also remove the warning re: "Consecutive odd pages found".

%% Adds the main heading for the SI text. Comment out this line if you do not have any supporting information text.

\section{Model Details}


\subsection{Model 1: Same--different relation with feed-forward  networks}\label{sec:model1}

 Our model of equality is given by \dasheg{eq:x2h-supp}{eq:h2y-supp}:
%
\begin{align}
  h &= \ReLU([a;b]W_{xh} + b_{h}) \label{eq:x2h-supp}\\
  y &= \softmax(hW_{hy} + b_{y}) \label{eq:h2y-supp}
\end{align}
%
\begin{figure}[H]
  \centering
  \resizebox{125pt}{!}{%
    \begin{tikzpicture}[
      % GLOBAL CFG
      font=\sf \large,
      >=LaTeX,
      % Styles
      rep/.style={% For representations
        rectangle,
        rounded corners=3mm,
        draw,
        very thick,
        minimum height =1cm,
        minimum width=1.61cm
      },
      function/.style={%For functions
        ellipse,
        draw,
        inner sep=1pt
      },
      gt/.style={% For internal inputs
        rectangle,
        draw,
        minimum width=5mm,
        minimum height=4mm,
        inner sep=1pt
      },
      function/.style={
        rectangle,
        draw,
        minimum width=5mm,
        minimum height=4mm,
        inner sep=1pt
      },
      arrowconcat/.style={% Arrows for concatenation
        rounded corners=.25cm,
        dashed,
        thick,
        ->,
      },
      arrowfunction/.style={% Arrows for concatenation
        rounded corners=.25cm,
        thick,
        ->,
      }
      ]

      % Start drawing the thing...
      % Draw the cell:
      \node [rep, fill=\inputcolor] (input1) at (-1.5,1.5){$x_1$} ;
      \node [rep, fill=\inputcolor] (input2) at (1.5,1.5){$x_2$} ;
      \node [rep, fill=\inputcolor] (concat) at (0,3){$x_1;x_2$} ;
      \draw [arrowconcat] (input1) -- (concat);
      \draw [arrowconcat] (input2) -- (concat);


      \node [rep, fill=\hiddencolor] (hidden) at (0,5){$h$} ;
      \node [gt, minimum width=1cm] (relu) at (0,4-0.1) {$\ReLU$};
      \draw [arrowfunction] (concat) -- (relu) -- (hidden);


      \node [rep,fill=\outputcolor] (output) at (0,7){$y$} ;
      \node [gt, minimum width=1cm] (softmax) at (0,6-0.1) {$\softmax$};
      \draw [arrowfunction] (hidden) -- (softmax) -- (output);

    \end{tikzpicture}
  }
  \caption{A single layer network computing equality.}
  \label{fig:models:equality}
\end{figure}


The input is a pair of vectors $(a,b)$, each of dimension $m$, which correspond to two stimulus objects. These vectors are non-featural representations that do not have features encoding properties of the objects or their identities. These are concatenated to form a single vector $[a;b]$ of dimension $2m$, which is the simplest way of merging the two representations to form a single input.

This representation is multiplied by a matrix of weights $W_{xh}$ of dimension $2m \times n$ and a bias vector $b_{h}$ of dimension $n$ is added to this result, where $n$ is the hidden layer dimensionality. These two steps create a linear projection of the input representation, and the bias term is the value of this linear projection when the input representation is the zero vector. Then, the non-linear activation function $\ReLU$ ($\ReLU(x) = \max(0, x)$) is applied element-wise to this linear projection. This non-linearity is what gives the neural model more expressive power than a logistic regression \citep{Cybenko:1989,Hornik:Stinchcombe:White:1989}. The result is the hidden representation $h$.

The hidden representation is the input to the classification layer: $h$ is multiplied by a second matrix of weights $W_{hy}$, dimension $n \times 2$, and a bias term $b_{y}$ (dimension 2) is added to this. This second bias term encodes the probabilities of each class when the hidden representation is 0. The result is fed through the softmax activation function: $\softmax(x)_{i} = \frac{\exp{x_{i}}}{\sum_{j} \exp{x_{j}}}$. This creates a probability distribution over the classes (positive and negative). For a given input, the model computes this probability distribution and the input is categorized as the class with the higher probability.

The parameters $W_{xh}$, $W_{hy}$, $b_{y}$, and $b_{h}$ are learned using back propagation with a cross entropy function. \update{This function is defined as follows, for a set of $N$ examples and $K$ classes:
%
\begin{equation}\label{eq:crossent}
\max(\theta)
\quad
\frac{1}{N}
\sum_{i=1}^{N}
\sum_{k=1}^{K}
y^{i,k} \log(h_{\theta}(i)^{k})
\end{equation}
%
where $\theta$ abbreviates the model parameters ($W_{xh}$, $W_{hy}$, $b_{y}$, $b_{h}$), $y^{i,k}$ is the actual label for example $i$ and class $k$, and $h_{\theta}(i)^{k}$ is the corresponding prediction.}

\Figref{fig:models:equality} provides a visual depiction of the model. The gray boxes correspond to embedding representations, the purple box is the hidden representation $h$, and the red box is the output distribution $y$. Dotted arrows depict concatenation, and solid arrows depict the dense relations corresponding to the matrix multiplications (plus bias terms) in \dasheg{eq-supp:x2h}{eq-supp:h2y}.




\subsection{Model 2: Sequential same--different (ABA task)}\label{sec:model2}



The specific model we use for this is as follows:
%
\begin{align}
  h_{t} &= \LSTM(x_{t}, h_{t-1}) \label{eq:lstm-recur-supp}\\
  y_{t} &= h_{t}W + b\label{eq:lstm-predict-supp}
\end{align}
%
This holds for $t > 0$, and we set $h_{0} = \mathbf{0}$. $\LSTM$ is a long short-term memory cell \cite{hochreiter:1997}.

\begin{figure}[H]
  \centering
  \resizebox{400pt}{!}{%
    \begin{tikzpicture}[
      % GLOBAL CFG
      font=\sf \large,
      >=LaTeX,
      % Styles
      rep/.style={% For representations
        rectangle,
        rounded corners=3mm,
        draw,
        very thick,
        minimum height =1cm,
        minimum width=1.61cm
      },
      function/.style={%For functions
        ellipse,
        draw,
        inner sep=1pt
      },
      gt/.style={% For internal inputs
        rectangle,
        draw,
        minimum width=5mm,
        minimum height=4mm,
        inner sep=1pt
      },
      function/.style={
        rectangle,
        draw,
        minimum width=5mm,
        minimum height=4mm,
        inner sep=1pt
      },
      arrowconcat/.style={% Arrows for concatenation
        rounded corners=.25cm,
        dashed,
        thick,
        ->,
      },
      arrowfunction/.style={% Arrows for concatenation
        rounded corners=.25cm,
        thick,
        ->,
      }
      ]

      % Start drawing the thing...
      % Draw the cell:
      \node [rep, fill=\inputcolor] (input1) at (-10,4){$<s>$} ;
      \node [rep, fill=\inputcolor] (input2) at (-6,4){$A$} ;
      \node [rep, fill=\inputcolor] (input3) at (-2,4){$B$} ;
      \node [rep, fill=\inputcolor] (input4) at (2,4){$A$} ;

      \node [rep, fill=\hiddencolor] (hidden0) at (-10,6){$h_0$} ;
      \node [rep, fill=\hiddencolor] (hidden1) at (-6,6){$h_1$} ;
      \node [rep, fill=\hiddencolor] (hidden2) at (-2,6){$h_2$} ;
      \node [rep, fill=\hiddencolor] (hidden3) at (2,6){$h_3$} ;
      \node [rep, fill=\hiddencolor] (hidden4) at (6,6){$h_4$} ;

      \node [gt] (LSTM0) at (-8,6){$\LSTM$} ;
      \node [gt] (LSTM1) at (-4,6){$\LSTM$} ;
      \node [gt] (LSTM2) at (0,6){$\LSTM$} ;
      \node [gt] (LSTM3) at (4,6){$\LSTM$} ;

      \node [rep, fill=\outputcolor] (output1) at (-6,8){$y_1$} ;
      \node [rep, fill=\outputcolor] (output2) at (-2,8){$y_2$} ;
      \node [rep, fill=\outputcolor] (output3) at (2,8){$y_3$} ;
      \node [rep, fill=\outputcolor] (output4) at (6,8){$y_4$} ;

      \node [gt] (compare1) at (-6,9.5){$\compare$} ;
      \node [gt] (compare2) at (-2,9.5){$\compare$} ;
      \node [gt] (compare3) at (2,9.5){$\compare$} ;
      \node [gt] (compare4) at (6,9.5){$\compare$} ;

      \node [rep, fill=\colorlabel] (label1) at (-6,11){$A$} ;
      \node [rep, fill=\colorlabel] (label2) at (-2,11){$B$} ;
      \node [rep, fill=\colorlabel] (label3) at (2,11){$A$} ;
      \node [rep, fill=\colorlabel] (label4) at (6,11){$</s>$} ;

      \draw [arrowfunction] (label1) -- (compare1);
      \draw [arrowfunction] (label2) -- (compare2);
      \draw [arrowfunction] (label3) -- (compare3);
      \draw [arrowfunction] (label4) -- (compare4);

      \draw [arrowfunction] (output1) -- (compare1);
      \draw [arrowfunction] (output2) -- (compare2);
      \draw [arrowfunction] (output3) -- (compare3);
      \draw [arrowfunction] (output4) -- (compare4);

      \draw [arrowfunction] (hidden1) -- (output1);
      \draw [arrowfunction] (hidden2) -- (output2);
      \draw [arrowfunction] (hidden3) -- (output3);
      \draw [arrowfunction] (hidden4) -- (output4);

      \draw [thick] (input1) to[out=0,in=-180, distance=1cm] (LSTM0);
      \draw [thick] (input2) to[out=0,in=-180, distance=1cm] (LSTM1);
      \draw [thick] (input3) to[out=0,in=-180, distance=1cm] (LSTM2);
      \draw [thick] (input4) to[out=0,in=-180, distance=1cm] (LSTM3);

      \draw [arrowfunction] (hidden0) -- (LSTM0) -- (hidden1);
      \draw [arrowfunction] (hidden1) -- (LSTM1)-- (hidden2);
      \draw [arrowfunction] (hidden2) -- (LSTM2)-- (hidden3);
      \draw [arrowfunction] (hidden3) -- (LSTM3)-- (hidden4);

      \draw[->,dashed,thick] (label1) to[out=0,in=-90, distance=1.99cm] (input2);
      \draw[->,dashed,thick] (label2) to[out=0,in=-90, distance=1.99cm] (input3);
      \draw[->,dashed,thick] (label3) to[out=0,in=-90, distance=1.99cm] (input4);
    \end{tikzpicture}
  }
  \caption{A recursive LSTM network producing ABA sequences.}
  \label{fig:reps:sequence}
\end{figure}

The input is a sequence of vectors $x_1, x_2, x_3, \dots$, each of dimension $m$, which correspond to a sequence of stimulus objects. These vectors are, again, non-featural representations that do not have features encoding properties of the objects or their identity.

At each timestep $t$, the input vector $x_t$ is fed into the $\LSTM$ cell along with the previous hidden representation $h_{t-1}$. The defining feature of an $\LSTM$ is the ability to decide whether to store information from the current input, $x_t$, and whether to remember or forget the information from the previous timestep $h_{t-t}$. The output of the $\LSTM$ cell is the hidden representation for the current time step $h_t$. The dimension of the hidden representations is $n$. The hidden representation is multiplied by a matrix $W$ with dimensionality $n \times m$ to produce $y_t$. This result, $y_t$, is a linear projection of the hidden representation into the input vector space, which is necessary because $y_t$ is a prediction of what the next input, $x_{t+1}$, will be.

The objective function is as follows:
%
\begin{equation}
\max(\theta)
\quad
-\frac{1}{N}
\sum_{i=1}^{N}
\sum_{t=1}^{T_{i}}
\left\| h_{\theta}\left(x^{i, 0:{t-1}}\right) - x^{i,t} \right\|^{2}
\end{equation}
%
for $N$ examples. Here, $T_{i}$ is the length of example $i$. As before, $\theta$ abbreviates the parameters of the model as specified in \dasheg{eq:lstm-recur-supp}{eq:lstm-predict-supp}. We use $h_{\theta}(x^{i, 0:{t-1}})$ for the vector predicted by the model for example~$i$ at timestep~$t$, which is compared to the actual vector at timestep $t$ via squared Euclidean distance (i.e., the mean squared error).

\Figref{fig:reps:sequence} depicts this model. At each timestep $t$, a vector $y_{t}$ (red) is predicted based on the input representation at $t$ (gray) and the hidden representation at $t$ (purple). During training, this is compared with the actual vector for timestep $t+1$ (green). During testing, the predicted vector $y_{i}$ is compared with every item in the union of the train and assessment vocabularies, and the closest vector (according to Euclidean distance) is taken to be the prediction. This vector is then used as the input for timestep $t+1$.



\subsection{Model 3a: A single layer feed forward network }
 The only change required to equations \dasheg{eq:x2h-supp}{eq:h2y-supp} is that we create inputs $[a;b;c;d]$: the flat concatenation of all the elements of the two pair of vectors. This change in turn leads $W_{xh}$ to have dimensionality $4m \times n$. The objective function is again defined using a cross entropy function, as in equation \ref{eq:crossent}. \Appref{app:model1-premack} provides a full picture of these learning trends. These models are able to find nearly perfect solutions, but vastly more training data is required for this task than was required for simple equality, and the network configuration matters much more. For example, our model with 10-dimensional entity representations and 100-dimensional hidden representations reached near perfect accuracy, but only with over 95,000 training instances. A comparable model with 50-dimensional entity representations failed to get traction at all with this amount of training data, and pretraining led to only minor improvements. For this reason, we also perform experiments with a deeper neural network.


\subsection{Model 3b: A deeper feed-forward network for hierarchical same--different}\label{sec:model3a}



This model extends \dasheg{eq:x2h-supp}{eq:h2y-supp} with an additional hidden layer and a larger input dimensionality, corresponding to the input pair of pairs $((a,b), (c,d))$ being flattened into a single concatenated representation $[a;b;c;d]$:
%
\begin{align}
  h_{1} &= \ReLU([a;b;c;d]W_{xh} + b_{h_{1}}) \label{eq:x2h1-supp}\\
  h_{2} &= \ReLU(h_{1}W_{hh} + b_{h_{2}}) \label{eq:x2h2-supp}\\
  y &= \softmax(h_{2}W_{hy} + b_{y}) \label{eq:h2y2-supp}
\end{align}
%

\begin{figure}[H]
  \centering
  \resizebox{200pt}{!}{%
    \begin{tikzpicture}[
      % GLOBAL CFG
      font=\sf \large,
      >=LaTeX,
      % Styles
      rep/.style={% For representations
        rectangle,
        rounded corners=3mm,
        draw,
        very thick,
        minimum height =1cm,
        minimum width=1.61cm
      },
      function/.style={%For functions
        ellipse,
        draw,
        inner sep=1pt
      },
      gt/.style={% For internal inputs
        rectangle,
        draw,
        minimum width=5mm,
        minimum height=4mm,
        inner sep=1pt
      },
      function/.style={
        rectangle,
        draw,
        minimum width=5mm,
        minimum height=4mm,
        inner sep=1pt
      },
      arrowconcat/.style={% Arrows for concatenation
        rounded corners=.25cm,
        dashed,
        thick,
        ->,
      },
      arrowfunction/.style={% Arrows for concatenation
        rounded corners=.25cm,
        thick,
        ->,
      }
      ]

      % Start drawing the thing...
      % Draw the cell:
      \node [rep, fill=\inputcolor] (input1) at (-3,1.5){$x_1$} ;
      \node [rep, fill=\inputcolor] (input2) at (-1,1.5){$x_2$} ;
      \node [rep, fill=\inputcolor] (input3) at (1,1.5){$x_3$} ;
      \node [rep, fill=\inputcolor] (input4) at (3,1.5){$x_4$} ;
      \node [rep, fill=\inputcolor] (concat) at (0,3){$x_1;x_2;x_3;x_4$} ;
      \draw [arrowconcat] (input1) -- (concat);
      \draw [arrowconcat] (input2) -- (concat);
      \draw [arrowconcat] (input3) -- (concat);
      \draw [arrowconcat] (input4) -- (concat);


      \node [rep, fill=\hiddencolor] (hidden) at (0,5){$h_1$} ;
      \node [gt, minimum width=1cm] (relu) at (0,4-0.1) {$\ReLU$};
      \draw [arrowfunction] (concat) -- (relu) -- (hidden);

      \node [rep, fill=\hiddencolortwo] (hidden2) at (0,7){$h_2$} ;
      \node [gt, minimum width=1cm] (relu2) at (0,6-0.1) {$\ReLU$};
      \draw [arrowfunction] (hidden) -- (relu2) -- (hidden2);


      \node [rep,fill=\outputcolor] (output) at (0,9){$y$} ;
      \node [gt, minimum width=1cm] (softmax) at (0,8-0.1) {$\softmax$};
      \draw [arrowfunction] (hidden2) -- (softmax) -- (output);

    \end{tikzpicture}
  }
  \caption{A two layer network computing hierarchical equality.}
  \label{fig:models:premack-deep}
\end{figure}


The objective function is again defined using a cross entropy function, as in equation \ref{eq:crossent}.

\Figref{fig:models:premack-deep} depicts this model.




\subsection{Model 3c: Pretraining for hierarchical same--different}\label{sec:model3b}



Our pretraining model is as follows:
%
\begin{align}
  h_1 &= \ReLU([a;b]W_{xh} + b_{h}) \label{eq:preh1-supp}\\
  h_2 &= \ReLU([c;d]W_{xh} + b_{h})\\
  h_3 &= \ReLU([h_1;h_2]W_{xh} + b_{h}) \\
  y &= \softmax(h_3W_{hy} + b_{y}) \label{eq:prey-supp}\
\end{align}
%

\begin{figure}[H]
  \centering
  \resizebox{300pt}{!}{%
    \begin{tikzpicture}[
      % GLOBAL CFG
      font=\sf \large,
      >=LaTeX,
      % Styles
      rep/.style={% For representations
        rectangle,
        rounded corners=3mm,
        draw,
        very thick,
        minimum height =1cm,
        minimum width=1.61cm
      },
      function/.style={%For functions
        ellipse,
        draw,
        inner sep=1pt
      },
      gt/.style={% For internal inputs
        rectangle,
        draw,
        minimum width=5mm,
        minimum height=4mm,
        inner sep=1pt
      },
      function/.style={
        rectangle,
        draw,
        minimum width=5mm,
        minimum height=4mm,
        inner sep=1pt
      },
      arrowconcat/.style={% Arrows for concatenation
        rounded corners=.25cm,
        dashed,
        thick,
        ->,
      },
      arrowfunction/.style={% Arrows for concatenation
        rounded corners=.25cm,
        thick,
        ->,
      }
      ]

      % Start drawing the thing...
      % Draw the cell:
      \node [rep, fill=\inputcolor] (input1) at (-4.5,1.5){$x_1$} ;
      \node [rep, fill=\inputcolor] (input2) at (-1.5,1.5){$x_2$} ;
      \node [rep, fill=\inputcolor] (concat) at (-3,3){$x_1;x_2$} ;
      \draw [arrowconcat] (input1) -- (concat);
      \draw [arrowconcat] (input2) -- (concat);


      \node [rep, fill=\hiddencolor] (hidden) at (-3,5){$h_1$} ;
      \node [gt, minimum width=1cm] (relu1) at (-3,4-0.1) {$\ReLU$};
      \draw [arrowfunction] (concat) -- (relu1) -- (hidden);


      \node [rep, fill=\inputcolor] (input3) at (4.5,1.5){$x_3$} ;
      \node [rep, fill=\inputcolor] (input4) at (1.5,1.5){$x_4$} ;
      \node [rep, fill=\inputcolor] (concat2) at (3,3){$x_3;x_4$} ;
      \draw [arrowconcat] (input3) -- (concat2);
      \draw [arrowconcat] (input4) -- (concat2);


      \node [rep, fill=\hiddencolor] (hidden2) at (3,5){$h_2$} ;
      \node [gt, minimum width=1cm] (relu2) at (3,4-0.1) {$\ReLU$};
      \draw [arrowfunction] (concat2) -- (relu2) -- (hidden2);


      \node [rep,fill=\hiddencolor] (hiddenconcat) at (0,6){$h_1;h_2$};
      \draw [arrowconcat] (hidden2) -- (hiddenconcat);
      \draw [arrowconcat] (hidden) -- (hiddenconcat);


      \node [rep, fill=\hiddencolor] (hidden3) at (0,8){$h_3$} ;
      \node [gt, minimum width=1cm] (relu3) at (0,7-0.1) {$\ReLU$};
      \draw [arrowfunction] (hiddenconcat) -- (relu3) -- (hidden3);


      \node [rep,fill=\outputcolor] (output) at (0,10){$y$} ;
      \node [gt, minimum width=1cm] (softmax) at (0,9-0.1) {$\softmax$};
      \draw [arrowfunction] (hidden3) -- (softmax) -- (output);

    \end{tikzpicture}
  }
  \caption{A single layer network pretrained on equality computing hierarchical equality.}
  \label{fig:models:premack}
\end{figure}

where $W_{xh}$, $W_{hy}$, $b_h$, and $b_y$ are the parameters from the model in equations \dasheg{eq:x2h-supp}{eq:h2y-supp} already trained on basic equality. Crucially, the same parameters, $W_{xh}$ and $b_h$, are used three times: twice to compute representations encoding whether a pair of input entities are equal ($h_1$, $h_2$), and once to compute a representation ($h_{3}$) encoding whether the truth values encoded by $h_1$ and $h_2$ are equal. This final representation is then used to compute a probability distribution over two classes, and the class with the higher probability is predicted by the model.

The objective function is again defined using a cross entropy function, as in equation \ref{eq:crossent}.




\section{Pretraining}\label{app:pretraining}

Our pretraining model closely resembles the models used for our main experiments. It defines a feed-forward network with a multitask objective. For an example $i$ and task $j$:
%
\begin{align}
  h_{i} &= \ReLU\left( E[i]W_{xh} + b_{h} \right) \\
  y_{i,j} &= \softmax(h^{i}W_{hy}^{j} + b_{y}^{j})
\end{align}
%
where $E[i]$ is the vector representation for example $i$ in the embedding matrix $E$. The overall objective of the model is to maximize the sum of the task objective functions. For $N$ examples, $J$ tasks, and $K_{j}$ the number of classes for task $j$:
%
\begin{equation}
  \max(\theta)
  \quad
  \frac{1}{N}
  \sum_{i=1}^{N}
  \sum_{j=1}^{J}
  \sum_{k=1}^{K_{j}}
  y^{i,j,k} \log \left( h_{\theta}(i)^{j,k} \right)
\end{equation}
%
where $y^{i,j,k}$ is the correct label for example $i$ in task $j$ and $h_{\theta}(i)^{j,k}$ is the predicted value for example $i$ in task $j$.

For the experiments in the paper, we initialize $E$ randomly and then, for pretraining on $J$ tasks, we create a random binary vector of length $J$ for each row in $E$. Each dimension (task) in $J$ is independent of the others.

Our motivation for pretraining is to update the embedding $E$ so that its representations contain rich structure that can be used by subsequent models. To achieve this, we backpropagate errors through the network and into $E$.

In our experiments, we always pretrain for 10 epochs where each epoch consists of an example for each item in the vocabulary. This choice is motivated primarily by computational costs; additional pretraining epochs greatly increase experiment run-times, though they do have the potential to imbue the representations with even more useful structure. Pretraining with the optimal hyperparameter settings always led to perfect accuracy on the pretraining tasks.


\section{Model optimization details}\label{app:optimization}

The feed forward networks for basic and hierarchical equality were implemented using the multi-layer perception from sklearn and a cross entropy function was used to compute the prediction error. The recursive LSTM network for the sequential ABA task was implemented using PyTorch and a mean squared error function was used to compute the prediction error. The networks for pretraining representations and for the hierarchical equality task were also implemented using PyTorch, with cross-entropy loss functions used to compute the prediction errors. For all models, Adam optimizers \citep{Kingma:Ba:2015} were used. For all models, we used a batch size of 1 and ran a hyperparameter search over learning rate values of \{0.00001, 0.0001, 0.001\} and l2 normalization values of \{0.0001, 0.001, 0.01\} for each hidden dimension and input dimension. We considered hidden dimensions of \{2, 10, 25, 50, 100\} and input dimensions of \{2, 10, 25, 50, 100\}. In the main text, we graph results for the single best hyperparameter setting. Later in the supplemental material under the heading 'Additional results plots', we show how model performance is affected by changes in hidden dimensionality and input dimensionality.


\section{Localist and binary feature representations prevent generalization}\label{app:generalization}

The method of representation impacts whether there is a natural notion of similarity between entities and the ability of models to generalize to examples unseen in training. These two attributes are deeply related; if there is a natural notion of similarity between vector representations, then models can generalize to inputs with representations that are similar to those seen in training.

In order to discuss how representation impacts generalization, we will need explain some properties of how neural models are trained. Standard neural models, including all models in the paper, begin with applying a linear layer to the input vector where no two input units are connected to the same weight. An easily observed fact about the back-propagation learning algorithm is that, if a unit of the input vector is always zero during training, then any weights connected to that unit and only that unit will not change from their initialized values during training. This means that, when a standard neural model is evaluated on an input vector that has a non-zero value for a unit that was zero throughout training, untrained weights are used and behavior is unpredictable.

Localist representations are orthogonal and equidistant from one another so there is no notion of similarity and consequently standard neural models have no ability to generalize to new examples. No two representations share a non-zero unit, and so when models are presented with inputs unseen in training, untrained weights are used and the resulting behavior is again unpredictable.

Property representations with binary features also limit generalization, though less severely than localist representations. Localist representations prevent generalization to entities unseen during training, while binary feature representations prevent generalization to features unseen during training. For example, if color and shape are represented as binary features, and a red square and blue circle are seen in training, then a model could generalize to the unseen entities of a blue circle or a red square. However, if no entity that is a circle is seen during training, then the binary feature representing the property of being a circle is zero throughout training and untrained weights are used when the model is presented with a entity that is a circle during testing, which results in unpredictable behavior.

Property representations with analog features do not inhibit generalization in the same way. If height is represented as an analog feature, then a single unit represents all height values and is always non-zero. Non-featural representations similarly do not inhibit generalization, because all units for all representations are non-zero and the network can learn parameters that create complex associations between these entities and its task labels.


\section{An analytic solution to identity with a feed forward network}\label{app:equality-solution}

We now show that our feed forward networks can solve the same--different problem we pose, in the following sense: for any set of inputs, we can find parameters $\theta$ that perfectly classify those inputs. At the same time, we also show that there are always additional inputs for which $\theta$ makes incorrect predictions.

Here are the parameters of a feed forward neural network that performs a binary classification task
%
\[ \texttt{ReLu}(\begin{pmatrix} x_1 \\ x_2  \end{pmatrix}^T \begin{pmatrix} W^{11} & W^{12}\\ W^{21}& W^{22} \end{pmatrix}) \begin{pmatrix} v^{11} & v^{12} \\ v^{21} & v^{22} \end{pmatrix} + \begin{pmatrix}b_1 &b_2 \end{pmatrix}= \begin{pmatrix} o_1 & o_2\end{pmatrix}\]
%
where, if $n$ is the dimension of entity embeddings used, then
%
\begin{align*}
  x, y,v^{11}, v^{12}, v^{21}, v^{22} &\in \mathbb{R}^{n \times 1} \\
  W^{11}, W^{12},W^{21}, W^{22} &\in \mathbb{R}^{n \times n} \\
  b_1, b_2, o_1, o_2 &\in \mathbb{R}
\end{align*}
%
Given an input $(x_1,x_2)$, if the output $o_1$ is larger than $o_2$, then one class is predicted; if the output $o_2$ is larger that $o_1$, then the other class is predicted. When the two outputs are equal, the network has predicted that both classes are equally likely and we can arbitrarily decide which class is predicted. In this case, the output $o_1$ predicts the two inputs, $x_1$ and $x_2$, are in the identity relation and the output $o_2$ predicts the two inputs are not. Now we specify parameters to provide an analytic solution to the identity relation using this network
%
\[ \texttt{ReLu}(\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}^T \begin{pmatrix} I & -I\\ -I& I \end{pmatrix}) \begin{pmatrix} \vec{1} & \vec{0} \\ \vec{1} & \vec{0} \end{pmatrix} + \begin{pmatrix}b_1 &b_2 \end{pmatrix}= \begin{pmatrix} o_1 & o_2\end{pmatrix}\]
%
where $I$ is the identity matrix, $-I$ is the negative identity matrix, and $\vec{1}$ and $\vec{0}$ are the two vectors in $\mathbb{R}^n$ that have all zeros and all ones, respectively. The output values, given an input, are
%
\[ o_1 = \sum_{i = 1}^{n}|(x_1)_i- (x_2)_i|+ b_1 \qquad \quad  o_2 = b_2\]
%
where two parameters are left unspecified, $b_1, b_2$. We present a visualization in Figure~\ref{fig:analyticff} of how the analytic solution to identity of this network changes depending on the values of two bias terms. In this example, the network receives two one-dimensional inputs, $x_1$ and $x_2$. If the ordered pair of inputs is in the shaded area on the graph, then they are predicted to be in the identity relation. If in the unshaded area, they are predicted not to be. The dotted line is where the network predicts the two classes to be equally likely.


\begin{figure}[h]
  \centering
  \newcommand\X{2}
  \newcommand\E{0.03}
  % \tcbox{
  \begin{tikzpicture}[scale=0.7]
    \centering
    \filldraw[fill=black, opacity=0.1]
    (-5,-5 + \X)--(5-\X,5) -- (5,5) -- (5,5 - \X)--(-5+ \X,-5)--(-5,-5)--(-5,-5 + \X);
    \draw[<->,ultra thick] (-5,0)--(5,0) node[right]{$x_1$};
    \draw[<->,ultra thick] (0,-5)--(0,5) node[above]{$x_2$};
    \draw[<->,thick] (-5,-5)--(5,5) ;
    \draw[<->,thick, dashed] (-5,-5 + \X)--(5-\X,5) ;
    \draw[<->,thick,dashed] (-5+ \X,-5)--(5,5 - \X) ;
    \draw [decorate,decoration={brace,amplitude=12pt},xshift=-0pt,yshift=0pt]
    (3 - \X/2 + \E ,3 + \X/2- \E) -- (3-\E,3+\E) node [black,midway, xshift=12pt,yshift=12pt]
    { \rotatebox{-45}{$\scriptstyle b_1 - b_2$}};
  \end{tikzpicture}
  % }
  \caption{A visual representation of how the analytic solution to identity of a single layer feed forward network changes depending on the values of two bias terms, $b_1,b_2$.}
  \label{fig:analyticff}
\end{figure}

The network predicts $x_1$ and $x_2$ to be in the identity relation if $\sum_{i = 1}^{n}|x_i- y_i| < b_1-b_2$ which is visualized as the points between two parallel lines above and below the solution line $x_1 = x_2$. As the difference $b_1-b_2$ gets smaller and smaller, the two lines that bound the network's predictions get closer and closer to the solution line. However, as long as $b_1-b_2$ is positive, there will always be inputs of the form $(r,r+(b_1-b_2)/2)$ that are false positives. For any set of inputs, we can find bias values that result in the network correctly classifying those inputs, but for any bias values, we can find an input that is incorrectly classified by those values. In other words, we have an arbitrarily good solution that is never perfect. We provide a proof below that there is no perfect solution and so this is the best outcome possible. However, if we were to decide that, if the network predicts that an input is equally likely in either class, then this input is predicted to be in the identity relation, we could have a perfect solution with $b_1= b_2$.

Here is proof that a perfect solution is not possible. A basic fact from topology is that the set $\{x: f(x) < g(x)\}$ is an open set if $f$ and $g$ are continuous functions. Let $N_{o_1}$ and $N_{o_2}$ be the functions that map an input $(x_1,x_2)$ to the output values of the neural network, $o_1$ and $o_2$, respectively. These functions are continuous. Consequently, the set $C = \{(x_1,x_2): N_{o_2}(x_1,x_2) < N_{o_1}(x_1,x_2)\}$, which is the set of inputs that are predicted to be in the equality relation, is open.

With this fact, we can show that, if the neural network correctly classifies any point on the solution line $x_1 = x_2$, then it must incorrectly classify some point not on the solution line. Suppose that $C$ contains some point $(x,x)$. Then, by the definition of an open set, $C$ contains some $\epsilon$ ball around $(x,x)$, and therefore $C$ contains $(x,x+\epsilon)$, which is not on the solution line $x_1=x_2$. Thus, $C$ can never be equal to the set $\{(x_1,x_2):x_1=x_2\}$. So, because $C$ is the set of inputs classified as being in the equality relation by the neural network, a perfect solution cannot be achieved. Thus, we can conclude our arbitrarily good solution is the best we can do.


\section{An analytic solution to ABA sequences}\label{sec:analyticlm}


Here are the parameters of a long short term memory recursive neural network (LSTM):
%
\begin{align*}
  f_t &= \sigma(x_t W_f  + h_{t-1} U_f  + b_f) \\
  i_t &= \sigma(x_t W_i  + h_{t-1} U_i + b_i) \\
  o_t &= \sigma(x_t W_o +  h_{t-1} U_o + b_o)\\
  c_t &= f_t \circ c_{t-1} + i_t \circ \ReLU(x_tW_c + h_{t-1}U_c + b_c) \\
  h_t &= o_t \circ \ReLU(c_t) \\
  y_t &= h_tV
\end{align*}
%
where, if $n$ is the representation size and $d$ is the network hidden dimension, then
%
\begin{align*}
  x_t \in \mathbb{R}^n, f_t, i_t, o_t,  h_t, c_t &\in \mathbb{R}^d\\
  W \in \mathbb{R}^{n \times d}, U &\in \mathbb{R}^{d \times d}\\
  V \in \mathbb{R}^{d \times n}, b &\in \mathbb{R}^d
\end{align*}
%
and $\sigma$ is the sigmoid function. The initial hidden state $h_0$ and initial cell state $c_0$ are both set to be the zero vector. We say that an LSTM model with specified parameters has learned to produce ABA sequences if the following holds: when the network is seeded with some entity vector representation as its first input, $x_1$, then the output $y_1$ is not equal to $x_1$ and at the next time step the output $y_2$ is equal to $x_1$.

We let $d = 2n + 1$ and assign the following parameters, which provide an analytic solution to producing ABA sequences:
%
\begin{align*}
  f_t &= \sigma(x_t \textbf{0}_{n\times d} + h_{t-1}\textbf{0}_{d \times d} + \textbf{N}_{d}) \\
  i_t &= \sigma(x_t\textbf{0}_{n\times d} + h_{t-1}\begin{bmatrix} -4 \dots -4 \\ \textbf{0}_{2n\times n} \end{bmatrix}  + \textbf{N}_d)   \\
  o_t &= \sigma(x_t\textbf{0}_{n\times d} +  h_{t-1} \begin{bmatrix} 1 \dots 1 \\ \textbf{0}_{2n\times n} \end{bmatrix} + \textbf{0}_{d})\\
  c_t &= f_t \circ c_{t-1} + \\
  & i_t \circ \ReLU\left(x_t \begin{bmatrix} 0 &  & \\ \vdots & -I_{n \times n} & I_{n \times n}\\ 0 & &  \end{bmatrix} + h_{t-1}\textbf{0}_{d \times d} + \begin{bmatrix} N & 0 \dots 0\end{bmatrix}\right) \\
  h_t &= o_t \circ \ReLU(c_t)\\
  y &= h_t \begin{bmatrix} 0 \dots 0 \\ -I_{n \times n} \\ I_{n \times n}  \end{bmatrix}
\end{align*}
%
Where $\textbf{0}_{j\times k}$ is the $j \times k$ zero matrix, $\textbf{m}_k$ is a $k$ dimensional vector with each element having the value $m$, $I_{n\times n}$ is the $n \times n$ identity matrix, and $N$ is some very large number.  Now we show that these parameters achieve an increasingly good solution as $N$ increases. When a value involves the number $N$, we will simplify the computation by saying what that value is equal to as $N$ approaches infinity. We begin with an arbitrary input $x_1$ and the input and hidden state intialized to zero vectors:
%
\[
  h_0 = \textbf{0}_d \qquad c_0 = \textbf{0}_d
\]
%
The gates at the first time step are easy to compute, as the cell state and hidden state are zero vectors so the gates are equal to the sigmoid function applied to their respective bias vectors. The forget gate is completely open, the output gate is partially open, and the input gate is fully open:
%
\begin{align*}
  f_1 &= \sigma(\textbf{N}_d) \approx \textbf{1}_d \\
  o_1 &= \sigma(\textbf{0}_d) = \textbf{0.5}_d \\
  i_1 &=  \sigma(\textbf{N}_d) \approx \textbf{1}_d
\end{align*}
%
Then we compute the cell and hidden states at the first timestep. The cell state encodes the information of the input vector, so it can be used to recover the vector at a later time step and receives no information from the previous cell state despite the forget gate being open, because the previous cell state is a zero vector. The hidden state is the cell state scaled by one half.
%
\begin{align*}
  c_1 &= \textbf{1}_d\circ \textbf{0}_d+ \\
      & \textbf{1}_d \circ \ReLU\left(x_1\begin{bmatrix} 0 &  & \\ \vdots & -I_{n \times n} & I_{n \times n}\\ 0 & &  \end{bmatrix} + \begin{bmatrix} N & 0 \dots 0\end{bmatrix}\right) \\
      & = \ReLU\left( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix}\right) \\[1ex]
  h_1 &= \textbf{0.5}_d\ReLU \left(\ReLU( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix})\right) \\
      &= \textbf{0.5}_d\circ \ReLU( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix})
\end{align*}
%
At the next time step, the forget gate remains fully open, the output gate changes from partially open to fully open, and the input gate changes from fully open to fully closed:
%
\begin{align*}
  f_2 &= \textbf{1}_d \\[2ex]
  o_2 &= \sigma(x_2\textbf{0}_{n\times d} + h_{1}\begin{bmatrix} 1 \dots 1 \\ \textbf{0}_{2n\times n} \end{bmatrix}  + \textbf{0}_d) \\
  &= \sigma(\textbf{0.5}_d\circ \ReLU\left( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix}\right)\begin{bmatrix} 1 \dots 1 \\ \textbf{0}_{2n\times n} \end{bmatrix}  + \textbf{0}_d)  \\
  &=  \sigma(\textbf{0.5}_d\circ \textbf{N}_d) \approx \textbf{1}_d \\[2ex]
  i_2 &= \sigma(x_2\textbf{0}_{n\times d} + h_{1}\begin{bmatrix} -4 \dots -4 \\ \textbf{0}_{2n\times n} \end{bmatrix}  + \textbf{N}_d)  \\
  &= \sigma(\textbf{0.5}_d\circ \ReLU\left( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix}\right)\begin{bmatrix} -4 \dots -4 \\ \textbf{0}_{2n\times n} \end{bmatrix}  + \textbf{N}_d)   \\
  &=  \sigma(\textbf{0.5}_d\circ\textbf{-4N}_d + \textbf{N}_d) \approx \textbf{0}_d
\end{align*}
%
Then we compute the cell and hidden states for the second timestep. Because the forget gate is completely open and the input gate is completely closed, the cell state remains the same. Because the output gate is completely open, the hidden state is the same as the cell state.
%
\begin{align*}
  c_2 &= \textbf{1}_d \circ \ReLU \left( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix}\right) +\\
      & \textbf{0}_d\circ \ReLU(x_2\begin{bmatrix} 0 &  & \\ \vdots & -I_{n \times n} & I_{n \times n}\\ 0 & &  \end{bmatrix} + \begin{bmatrix} N & 0 \dots 0\end{bmatrix}) \\
      &= \ReLU \left( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix}\right) \\[2ex]
  h_2 &= \textbf{1}_d \circ \ReLU\left(\ReLU ( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix})\right) \\
      &= \ReLU \left( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix}\right)
\end{align*}
%
With the hidden states for the first and second time steps, we can compute the output values and find that the output at the first time step is the initial input vector scaled by one half and the output at the second time step is the initial input vector.

\begin{align*}
  y_1 &= h_1\begin{bmatrix} 0 \dots 0 \\ -I_{n \times n} \\ I_{n \times n}  \end{bmatrix} = \textbf{0.5}_d\circ \ReLU\left( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix}\right) \begin{bmatrix} 0 \dots 0 \\ -I_{n \times n} \\ I_{n \times n}  \end{bmatrix} \\
      &= \textbf{0.5}_d\circ x_1\\
  y_2 &= h_2\begin{bmatrix} 0 \dots 0 \\ -I_{n \times n} \\ I_{n \times n}  \end{bmatrix} = \texttt{ReLu} ( \begin{bmatrix} N & -x_1 & x_1 \end{bmatrix}) \begin{bmatrix} 0 \dots 0 \\ -I_{n \times n} \\ I_{n \times n}  \end{bmatrix} = x_1
\end{align*}
%
Then, because $y_1 = \textbf{0.5}_d\circ x_1 \not = x_1$ and $y_2 = x_1$, this network produces ABA sequences.




\section{Additional results plots}


\subsection{Model 1 for basic same--different}\label{app:model1-results}

\Figref{fig:model1} explores a wider range of hidden dimensionalities for Model~1 applied to the basic same--differerent task. The lines correspond to different embedding dimensionalities.

\begin{figure}[H]
  \centering

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/equality-train_size-embed_dim-hidden_dim=2.pdf}
    \caption{Hidden dimensionality 2.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/equality-train_size-embed_dim-hidden_dim=10.pdf}
    \caption{Hidden dimensionality 10.}
  \end{subfigure}

  \vspace{24pt}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/equality-train_size-embed_dim-hidden_dim=25.pdf}
    \caption{Hidden dimensionality 25.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/equality-train_size-embed_dim-hidden_dim=50.pdf}
    \caption{Hidden dimensionality 50.}
  \end{subfigure}

  \vspace{24pt}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/equality-train_size-embed_dim-hidden_dim=100.pdf}
    \caption{Hidden dimensionality 100.}
    \label{fig:model1-rep}
  \end{subfigure}
  \caption{Results for Model 1 for basic same--different. Lines correspond to different input dimensions. These models were provided random input representations.}
  \label{fig:model1}
\end{figure}




\subsection{Model 2 for sequential same--different}

\Figref{fig:model2} explores a wider range of hidden dimensionalities for Model~2 applied to the sequential ABA task. The lines correspond to different embedding dimensionalities. The full training set is presented to the model in multiple epochs.

\begin{figure}[H]
  \centering

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/fuzzy-lm-vocab20-pretrain-3tasks-train_size-embed_dim-hidden_dim=2.pdf}
    \caption{Hidden dimensionality 2.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/fuzzy-lm-vocab20-pretrain-3tasks-train_size-embed_dim-hidden_dim=10.pdf}
    \caption{Hidden dimensionality 10.}
  \end{subfigure}

  \vspace{24pt}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/fuzzy-lm-vocab20-pretrain-3tasks-train_size-embed_dim-hidden_dim=25.pdf}
    \caption{Hidden dimensionality 25.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/fuzzy-lm-vocab20-pretrain-3tasks-train_size-embed_dim-hidden_dim=50.pdf}
    \caption{Hidden dimensionality 50.}
  \end{subfigure}

  \vspace{24pt}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/fuzzy-lm-vocab20-pretrain-3tasks-train_size-embed_dim-hidden_dim=100.pdf}
    \caption{Hidden dimensionality 100.}
    \label{fig:model2-rep}
  \end{subfigure}

  \caption{Results for Model 2 for basic same--different, with a vocabulary size of 20. Lines correspond to different input dimensions. These models were provided random input representations.}
  \label{fig:model2}
\end{figure}




\subsection{Model 3a for hierarchical same--different}\label{app:model1-premack}

\Figref{fig:model1:premack} shows the results of applying the model in \dasheg{eq:x2h-supp}{eq:h2y-supp} to the hierarchical same--different task. The lines correspond to different embedding dimensionalities. The only change from that model is that the inputs have dimensionality $4m$, since the four distinct representations in task inputs are simply concatenated.


\begin{figure}[H]
  \centering

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h1-train_size-embed_dim-hidden_dim=2.pdf}
    \caption{Hidden dimensionality 2.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h1-train_size-embed_dim-hidden_dim=10.pdf}
    \caption{Hidden dimensionality 10.}
  \end{subfigure}

  \vspace{24pt}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h1-train_size-embed_dim-hidden_dim=25.pdf}
    \caption{Hidden dimensionality 25.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h1-train_size-embed_dim-hidden_dim=50.pdf}
    \caption{Hidden dimensionality 50.}
  \end{subfigure}

  \vspace{24pt}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h1-train_size-embed_dim-hidden_dim=100.pdf}
    \caption{Hidden dimensionality 100.}
  \end{subfigure}
  \caption{Results for Model 1 applied to the hierarchical same--different task. Lines correspond to different input dimensions. These models were provided random input representations.}
  \label{fig:model1:premack}
\end{figure}



\subsection{Model 3b for hierarchical same--different}

\Figref{fig:model3a} shows the results of applying the model in \dasheg{eq:x2h1-supp}{eq:h2y2-supp} to the hierarchical same--different task. The lines correspond to different embedding dimensionalities.

\begin{figure}[H]
  \centering

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h2-train_size-embed_dim-hidden_dim=2.pdf}
    \caption{Hidden dimensionality 2.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h2-train_size-embed_dim-hidden_dim=10.pdf}
    \caption{Hidden dimensionality 10.}
  \end{subfigure}

  \vspace{24pt}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h2-train_size-embed_dim-hidden_dim=25.pdf}
    \caption{Hidden dimensionality 25.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h2-train_size-embed_dim-hidden_dim=50.pdf}
    \caption{Hidden dimensionality 50.}
  \end{subfigure}

  \vspace{24pt}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=1\textwidth]{../fig/flatpremack-h2-train_size-embed_dim-hidden_dim=100.pdf}
    \caption{Hidden dimensionality 100.}
    \label{fig:model3a-rep}
  \end{subfigure}
  \caption{Results for Model 3a applied to the hierarchical same--different task. Lines correspond to different input dimensions. These models were provided random input representations.}
  \label{fig:model3a}
\end{figure}



%\subsection{Pretraining input representations and equality parameters}\label{app:double-pretrain}

%In \secref{sec:modular}, we showed that pretraining an entire basic equality network led to faster learning in the hierarchical same--different task. This kind of network pretraining can be combined with the input-level pretrained we explored elsewhere in the paper (details in \appref{app:pretraining}). \Figref{fig:double-pretrain} reports initial experiments for this combination of pretraining regimes. Although the results are not better than the ones we report in the paper, this still seems like a promising idea, though one for which optimal solutions might be hard to find.

%\begin{figure}[H]
%  \centering
%  \includegraphics[width=0.48\textwidth]{../fig/input-as-output-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
%  \caption{Results for the hierarchical same--different task for a model in which both the input representations and the basic equality network are pretrained. The `no pretrain' model is the best one from \figref{fig:premack-pretraining-results} (25-dimensional embeddings, 100-dimensional hidden representations). The input-pretrained models use this same configuration.}
%  \label{fig:double-pretrain}
%\end{figure}

\end{document}



















